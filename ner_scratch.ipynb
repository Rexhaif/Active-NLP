{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5c1dc9c850>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import copy\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_ner.models import CNN_BiLSTM_CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parse():\n",
    "    def __init__(self):\n",
    "        self.dataset = 'conll'\n",
    "        self.result_path = 'neural_ner/results'\n",
    "        self.usemodel = 'CNN_BiLSTM_CRF'\n",
    "        self.worddim = 100\n",
    "        self.pretrnd = 'wordvectors/glove.6B.100d.txt'\n",
    "        self.reload = 0\n",
    "        self.checkpoint = '.'\n",
    "        self.num_epochs = 25\n",
    "\n",
    "opt=Parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5410, -0.2934, -2.1788,  0.5684], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.autograd.Variable(torch.randn(4)).cuda()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import codecs\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'\n",
    "\n",
    "def get_name(parameters):\n",
    "    \"\"\"\n",
    "    Generate a model name from its parameters.\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for k, v in parameters.items():\n",
    "        if type(v) is str and \"/\" in v:\n",
    "            l.append((k, v[::-1][:v[::-1].index('/')][::-1]))\n",
    "        else:\n",
    "            l.append((k, v))\n",
    "    name = \",\".join([\"%s=%s\" % (k, str(v).replace(',', '')) for k, v in l])\n",
    "    return \"\".join(i for i in name if i not in \"\\/:*?<>|\")\n",
    "\n",
    "\n",
    "def set_values(name, param, pretrained):\n",
    "    \"\"\"\n",
    "    Initialize a network parameter with pretrained values.\n",
    "    We check that sizes are compatible.\n",
    "    \"\"\"\n",
    "    param_value = param.get_value()\n",
    "    if pretrained.size != param_value.size:\n",
    "        raise Exception(\n",
    "            \"Size mismatch for parameter %s. Expected %i, found %i.\"\n",
    "            % (name, param_value.size, pretrained.size)\n",
    "        )\n",
    "    param.set_value(np.reshape(\n",
    "        pretrained, param_value.shape).astype(np.float32))\n",
    "\n",
    "\n",
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "\n",
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "\n",
    "def iob2(tags):\n",
    "    \"\"\"\n",
    "    Check that tags have a valid IOB format.\n",
    "    Tags in IOB1 format are converted to IOB2.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        split = tag.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "        elif tags[i - 1][1:] == tag[1:]:\n",
    "            continue\n",
    "        else:  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "    return True\n",
    "\n",
    "\n",
    "def iob_iobes(tags):\n",
    "    \"\"\"\n",
    "    IOB -> IOBES\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'B':\n",
    "            if i + 1 != len(tags) and \\\n",
    "               tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('B-', 'S-'))\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            if i + 1 < len(tags) and \\\n",
    "                    tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('I-', 'E-'))\n",
    "        else:\n",
    "            raise Exception('Invalid IOB format!')\n",
    "    return new_tags\n",
    "\n",
    "\n",
    "def iobes_iob(tags):\n",
    "    \"\"\"\n",
    "    IOBES -> IOB\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag.split('-')[0] == 'B':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'S':\n",
    "            new_tags.append(tag.replace('S-', 'B-'))\n",
    "        elif tag.split('-')[0] == 'E':\n",
    "            new_tags.append(tag.replace('E-', 'I-'))\n",
    "        elif tag.split('-')[0] == 'O':\n",
    "            new_tags.append(tag)\n",
    "        else:\n",
    "            raise Exception('Invalid format!')\n",
    "    return new_tags\n",
    "\n",
    "\n",
    "def insert_singletons(words, singletons, p=0.5):\n",
    "    \"\"\"\n",
    "    Replace singletons by the unknown word with a probability p.\n",
    "    \"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word in singletons and np.random.uniform() < p:\n",
    "            new_words.append(0)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def pad_word_chars(words):\n",
    "    \"\"\"\n",
    "    Pad the characters of the words in a sentence.\n",
    "    Input:\n",
    "        - list of lists of ints (list of words, a word being a list of char indexes)\n",
    "    Output:\n",
    "        - padded list of lists of ints\n",
    "        - padded list of lists of ints (where chars are reversed)\n",
    "        - list of ints corresponding to the index of the last character of each word\n",
    "    \"\"\"\n",
    "    max_length = max([len(word) for word in words])\n",
    "    char_for = []\n",
    "    char_rev = []\n",
    "    char_pos = []\n",
    "    for word in words:\n",
    "        padding = [0] * (max_length - len(word))\n",
    "        char_for.append(word + padding)\n",
    "        char_rev.append(word[::-1] + padding)\n",
    "        char_pos.append(len(word) - 1)\n",
    "    return char_for, char_rev, char_pos\n",
    "\n",
    "\n",
    "def create_input(data, parameters, add_label, singletons=None):\n",
    "    \"\"\"\n",
    "    Take sentence data and return an input for\n",
    "    the training or the evaluation function.\n",
    "    \"\"\"\n",
    "    words = data['words']\n",
    "    chars = data['chars']\n",
    "    if singletons is not None:\n",
    "        words = insert_singletons(words, singletons)\n",
    "    if parameters['cap_dim']:\n",
    "        caps = data['caps']\n",
    "    char_for, char_rev, char_pos = pad_word_chars(chars)\n",
    "    input = []\n",
    "    if parameters['word_dim']:\n",
    "        input.append(words)\n",
    "    if parameters['char_dim']:\n",
    "        input.append(char_for)\n",
    "        if parameters['char_bidirect']:\n",
    "            input.append(char_rev)\n",
    "        input.append(char_pos)\n",
    "    if parameters['cap_dim']:\n",
    "        input.append(caps)\n",
    "    if add_label:\n",
    "        input.append(data['tags'])\n",
    "    return input\n",
    "\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and mapping of characters, sorted by frequency.\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    dico['<PAD>'] = 10000000\n",
    "    # dico[';'] = 0\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    print(\"Found %i unique characters\" % len(dico))\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag\n",
    "\n",
    "\n",
    "def cap_feature(s):\n",
    "    \"\"\"\n",
    "    Capitalization feature:\n",
    "    0 = low caps\n",
    "    1 = all caps\n",
    "    2 = first letter caps\n",
    "    3 = one capital (not first letter)\n",
    "    \"\"\"\n",
    "    if s.lower() == s:\n",
    "        return 0\n",
    "    elif s.upper() == s:\n",
    "        return 1\n",
    "    elif s[0].upper() == s[0]:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "def prepare_sentence(str_words, word_to_id, char_to_id, lower=False):\n",
    "    \"\"\"\n",
    "    Prepare a sentence for evaluation.\n",
    "    \"\"\"\n",
    "    def f(x): return x.lower() if lower else x\n",
    "    words = [word_to_id[f(w) if f(w) in word_to_id else '<UNK>']\n",
    "             for w in str_words]\n",
    "    chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "             for w in str_words]\n",
    "    caps = [cap_feature(w) for w in str_words]\n",
    "    return {\n",
    "        'str_words': str_words,\n",
    "        'words': words,\n",
    "        'chars': chars,\n",
    "        'caps': caps\n",
    "    }\n",
    "\n",
    "\n",
    "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=True):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    def f(x): return x.lower() if lower else x\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[f(w) if f(w) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        # Skip characters that are not in the training set\n",
    "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "                 for w in str_words]\n",
    "        caps = [cap_feature(w) for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'chars': chars,\n",
    "            'caps': caps,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "\n",
    "def augment_with_pretrained(dictionary, ext_emb_path, words):\n",
    "    \"\"\"\n",
    "    Augment the dictionary with words that have a pretrained embedding.\n",
    "    If `words` is None, we add every word that has a pretrained embedding\n",
    "    to the dictionary, otherwise, we only add the words that are given by\n",
    "    `words` (typically the words in the development and test sets.)\n",
    "    \"\"\"\n",
    "    print('Loading pretrained embeddings from %s...' % ext_emb_path)\n",
    "    assert os.path.isfile(ext_emb_path)\n",
    "\n",
    "    # Load pretrained embeddings from file\n",
    "    pretrained = set([\n",
    "        line.rstrip().split()[0].strip()\n",
    "        for line in codecs.open(ext_emb_path, 'r', 'utf-8')\n",
    "        if len(ext_emb_path) > 0\n",
    "    ])\n",
    "    \n",
    "    if words is None:\n",
    "        for word in pretrained:\n",
    "            if word not in dictionary:\n",
    "                dictionary[word] = 0\n",
    "    else:\n",
    "        for word in words:\n",
    "            if any(x in pretrained for x in [\n",
    "                word,\n",
    "                word.lower(),\n",
    "                re.sub('\\d', '0', word.lower())\n",
    "            ]) and word not in dictionary:\n",
    "                dictionary[word] = 0\n",
    "\n",
    "    word_to_id, id_to_word = create_mapping(dictionary)\n",
    "    return dictionary, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def pad_seq(seq, max_length, PAD_token=0):\n",
    "    \n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq\n",
    "\n",
    "def log_sum_exp(vec, dim=-1, keepdim = False):\n",
    "    max_score, _ = vec.max(dim, keepdim=keepdim)\n",
    "    if keepdim:\n",
    "        stable_vec = vec - max_score\n",
    "    else:\n",
    "        stable_vec = vec - max_score.unsqueeze(dim)\n",
    "    output = max_score + (stable_vec.exp().sum(dim, keepdim=keepdim)).log()\n",
    "    return output\n",
    "\n",
    "def create_batches(dataset, batch_size, order='keep', str_words=False, tag_padded= True):\n",
    "    \n",
    "        newdata = copy.deepcopy(dataset)\n",
    "        if order=='sort':\n",
    "            newdata.sort(key = lambda x:len(x['words']))\n",
    "        elif order=='random':\n",
    "            random.shuffle(newdata)\n",
    "        \n",
    "        newdata = np.array(newdata)  \n",
    "        batches = []\n",
    "        num_batches = np.ceil(len(dataset)/float(batch_size)).astype('int')\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            batch_data = newdata[(i*batch_size):min(len(dataset),(i+1)*batch_size)]\n",
    "            \n",
    "            words_seqs = [itm['words'] for itm in batch_data]\n",
    "            caps_seqs = [itm['caps'] for itm in batch_data]\n",
    "            target_seqs = [itm['tags'] for itm in batch_data]\n",
    "            chars_seqs = [itm['chars'] for itm in batch_data]\n",
    "            str_words_seqs = [itm['str_words'] for itm in batch_data]\n",
    "            \n",
    "            seq_pairs = sorted(zip(words_seqs, caps_seqs, target_seqs, chars_seqs, str_words_seqs), \n",
    "                               key=lambda p: len(p[0]), reverse=True)\n",
    "            \n",
    "            words_seqs, caps_seqs, target_seqs, chars_seqs, str_words_seqs = zip(*seq_pairs)\n",
    "            words_lengths = np.array([len(s) for s in words_seqs])\n",
    "            \n",
    "            words_padded = np.array([pad_seq(s, np.max(words_lengths)) for s in words_seqs])\n",
    "            caps_padded = np.array([pad_seq(s, np.max(words_lengths)) for s in caps_seqs])\n",
    "            \n",
    "            if tag_padded:\n",
    "                target_padded = np.array([pad_seq(s, np.max(words_lengths)) for s in target_seqs])\n",
    "            else:\n",
    "                target_padded = target_seqs\n",
    "            \n",
    "            words_mask = (words_padded!=0).astype('int')\n",
    "            \n",
    "            chars_pseqs = [pad_seq(s, max(words_lengths), []) for s in chars_seqs]\n",
    "            chars_lengths = np.array([[len(s) for s in w] for w in chars_pseqs]).reshape(-1)\n",
    "            chars_padded = np.array([[pad_seq(s, np.max(chars_lengths)) \n",
    "                                      for s in w] for w in chars_pseqs]).reshape(-1,np.max(chars_lengths))\n",
    "    \n",
    "            if str_words:\n",
    "                outputdict = {'words':words_padded, 'caps':caps_padded, 'tags': target_padded, \n",
    "                              'chars': chars_padded, 'wordslen': words_lengths, 'charslen': chars_lengths,\n",
    "                              'tagsmask':words_mask, 'str_words': str_words_seqs}\n",
    "            else:\n",
    "                outputdict = {'words':words_padded, 'caps':caps_padded, 'tags': target_padded, \n",
    "                              'chars': chars_padded, 'wordslen': words_lengths, 'charslen': chars_lengths,\n",
    "                              'tagsmask':words_mask}\n",
    "            \n",
    "            batches.append(outputdict)\n",
    "        \n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "\n",
    "class Initializer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def init_embedding(self, input_embedding):\n",
    "        bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "        nn.init.uniform_(input_embedding, -bias, bias)\n",
    "    \n",
    "    def init_linear(self, input_linear):\n",
    "        bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
    "        nn.init.uniform_(input_linear.weight, -bias, bias)\n",
    "        if input_linear.bias is not None:\n",
    "            input_linear.bias.data.zero_()\n",
    "    \n",
    "    def init_lstm(self, input_lstm):\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "            bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform_(weight, -bias, bias)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "            bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform_(weight, -bias, bias)\n",
    "        \n",
    "        if input_lstm.bidirectional:\n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "                bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "                nn.init.uniform_(weight, -bias, bias)\n",
    "                weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "                bias = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "                nn.init.uniform_(weight, -bias, bias)\n",
    "        \n",
    "        if input_lstm.bias:\n",
    "            \n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                weight = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "                weight.data.zero_()\n",
    "                weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                weight = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "                weight.data.zero_()\n",
    "                weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "            if input_lstm.bidirectional:\n",
    "                for ind in range(0, input_lstm.num_layers):\n",
    "                    weight = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "                    weight.data.zero_()\n",
    "                    weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                    weight = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "                    weight.data.zero_()\n",
    "                    weight.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import codecs\n",
    "import pickle as cPickle\n",
    "\n",
    "class Loader(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def pad_sequence_cnn(self, chars):\n",
    "        d = {}\n",
    "        chars_length = [len(c) for c in chars]\n",
    "        chars_maxlen = max(chars_length)\n",
    "        chars_mask = np.zeros((len(chars_length), chars_maxlen), dtype='int')\n",
    "        for i, c in enumerate(chars):\n",
    "            chars_mask[i, :chars_length[i]] = c\n",
    "        return chars_mask, chars_length, d\n",
    "    \n",
    "    \n",
    "    def pad_sequence_rnn(self, chars):\n",
    "        chars_sorted = sorted(chars, key=lambda p: len(p), reverse=True)\n",
    "        d = {}\n",
    "        for i, ci in enumerate(chars):\n",
    "            for j, cj in enumerate(chars_sorted):\n",
    "                if ci == cj and not j in d and not i in d.values():\n",
    "                    d[j] = i\n",
    "                    continue\n",
    "        chars_length = [len(c) for c in chars_sorted]\n",
    "        chars_maxlen = max(chars_length)\n",
    "        chars_mask = np.zeros((len(chars_sorted), char_maxlen), dtype='int')\n",
    "        for i, c in enumerate(chars_sorted):\n",
    "            chars_mask[i, :chars_length[i]] = c\n",
    "        return chars_mask, chars_length, d\n",
    "    \n",
    "    def update_tag_scheme(self, sentences, tag_scheme):\n",
    "        \n",
    "        for i, s in enumerate(sentences):\n",
    "            tags = [w[-1] for w in s]\n",
    "            if not iob2(tags):\n",
    "                s_str = '\\n'.join(' '.join(w) for w in s)\n",
    "                raise Exception('Sentences should be given in IOB format! ' +\n",
    "                                'Please check sentence %i:\\n%s' % (i, s_str))\n",
    "            if tag_scheme == 'iob':\n",
    "                for word, new_tag in zip(s, tags):\n",
    "                    word[-1] = new_tag\n",
    "            elif tag_scheme == 'iobes':\n",
    "                new_tags = iob_iobes(tags)\n",
    "                for word, new_tag in zip(s, new_tags):\n",
    "                    word[-1] = new_tag\n",
    "            else:\n",
    "                raise Exception('Unknown tagging scheme!')\n",
    "                \n",
    "    def word_mapping(self, sentences, lower):\n",
    "        \n",
    "        words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "        dico = create_dico(words)\n",
    "\n",
    "        dico['<PAD>'] = 10000001\n",
    "        dico['<UNK>'] = 10000000\n",
    "        dico = {k:v for k,v in dico.items() if v>=3}\n",
    "        word_to_id, id_to_word = create_mapping(dico)\n",
    "\n",
    "        print(\"Found %i unique words (%i in total)\" % (\n",
    "            len(dico), sum(len(x) for x in words)\n",
    "        ))\n",
    "        return dico, word_to_id, id_to_word\n",
    "    \n",
    "    def load_conll_sentences(self, path, lower, zeros):\n",
    "        \n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in codecs.open(path, 'r', 'utf-8'):\n",
    "            line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "            if not line:\n",
    "                if len(sentence) > 0:\n",
    "                    if 'DOCSTART' not in sentence[0][0]:\n",
    "                        sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                word = line.split()\n",
    "                assert len(word) >= 2\n",
    "                sentence.append(word)\n",
    "        if len(sentence) > 0:\n",
    "            if 'DOCSTART' not in sentence[0][0]:\n",
    "                sentences.append(sentence)\n",
    "        return sentences\n",
    "    \n",
    "    def load_conll(self, dataset ,parameters):\n",
    "        \n",
    "        zeros = parameters['zeros']\n",
    "        lower = parameters['lower']\n",
    "        word_dim = parameters['wrdim']\n",
    "        pretrained = parameters['ptrnd']\n",
    "        tag_scheme = parameters['tgsch']\n",
    "        \n",
    "        train_path = os.path.join(dataset,'eng.train')\n",
    "        dev_path = os.path.join(dataset,'eng.testa')\n",
    "        test_path = os.path.join(dataset,'eng.testb')\n",
    "        test_train_path = os.path.join(dataset,'eng.train54019')\n",
    "        \n",
    "        train_sentences = self.load_conll_sentences(train_path, lower, zeros)\n",
    "        dev_sentences = self.load_conll_sentences(dev_path, lower, zeros)\n",
    "        test_sentences = self.load_conll_sentences(test_path, lower, zeros)\n",
    "        test_train_sentences = self.load_conll_sentences(test_train_path, lower, zeros)\n",
    "        \n",
    "        self.update_tag_scheme(train_sentences, tag_scheme)\n",
    "        self.update_tag_scheme(dev_sentences, tag_scheme)\n",
    "        self.update_tag_scheme(test_sentences, tag_scheme)\n",
    "        self.update_tag_scheme(test_train_sentences, tag_scheme)\n",
    "        \n",
    "        dico_words_train = self.word_mapping(train_sentences, lower)[0]\n",
    "        \n",
    "        all_embedding = 1\n",
    "        dico_words, word_to_id, id_to_word = augment_with_pretrained(\n",
    "                dico_words_train.copy(),\n",
    "                pretrained,\n",
    "                list(itertools.chain.from_iterable(\n",
    "                    [[w[0] for w in s] for s in dev_sentences + test_sentences])\n",
    "                ) if not all_embedding else None)\n",
    "\n",
    "        dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "        dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
    "        \n",
    "        train_data = prepare_dataset(train_sentences, word_to_id, char_to_id, tag_to_id, lower)\n",
    "        dev_data = prepare_dataset(dev_sentences, word_to_id, char_to_id, tag_to_id, lower)\n",
    "        test_data = prepare_dataset(test_sentences, word_to_id, char_to_id, tag_to_id, lower)\n",
    "        test_train_data = prepare_dataset(test_train_sentences, word_to_id, char_to_id, tag_to_id, lower)\n",
    "        \n",
    "        print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "              len(train_data), len(dev_data), len(test_data)))\n",
    "        \n",
    "        mapping_file = os.path.join(dataset,'mappinghawa.pkl')\n",
    "        \n",
    "        if not os.path.isfile(mapping_file):\n",
    "            all_word_embeds = {}\n",
    "            for i, line in enumerate(codecs.open(pretrained, 'r', 'utf-8')):\n",
    "                s = line.strip().split()\n",
    "                if len(s) == word_dim + 1:\n",
    "                    all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "            word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), word_dim))\n",
    "\n",
    "            for w in word_to_id:\n",
    "                if w in all_word_embeds:\n",
    "                    word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "                elif w.lower() in all_word_embeds:\n",
    "                    word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "            print('Loaded %i pretrained embeddings.' % len(all_word_embeds))\n",
    "\n",
    "            with open(mapping_file, 'wb') as f:\n",
    "                mappings = {\n",
    "                    'word_to_id': word_to_id,\n",
    "                    'tag_to_id': tag_to_id,\n",
    "                    'id_to_tag': id_to_tag,\n",
    "                    'char_to_id': char_to_id,\n",
    "                    'parameters': parameters,\n",
    "                    'word_embeds': word_embeds\n",
    "                }\n",
    "                cPickle.dump(mappings, f)\n",
    "        else:\n",
    "            mappings = cPickle.load(open(mapping_file,'rb'))\n",
    "            \n",
    "        return train_data, dev_data, test_data, test_train_data, mappings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: CNN_BiLSTM_CRF\n",
      "Dataset: conll\n",
      "Found 7518 unique words (203621 in total)\n",
      "Loading pretrained embeddings from wordvectors/glove.6B.100d.txt...\n",
      "Found 85 unique characters\n",
      "Found 11 unique named entity tags\n",
      "14041 / 3250 / 3453 sentences in train / dev / test.\n",
      "Load Complete\n"
     ]
    }
   ],
   "source": [
    "parameters = OrderedDict()\n",
    "\n",
    "parameters['model'] = opt.usemodel\n",
    "parameters['wrdim'] = opt.worddim\n",
    "parameters['ptrnd'] = opt.pretrnd\n",
    "\n",
    "if opt.usemodel == 'CNN_BiLSTM_CRF':\n",
    "    parameters['lower'] = 1\n",
    "    parameters['zeros'] = 0\n",
    "    parameters['cpdim'] = 0\n",
    "    parameters['dpout'] = 0.5\n",
    "    parameters['chdim'] = 25\n",
    "    parameters['tgsch'] = 'iob'\n",
    "\n",
    "    parameters['wldim'] = 200\n",
    "    parameters['cldim'] = 25\n",
    "    parameters['cnchl'] = 25\n",
    "    \n",
    "    parameters['lrate'] = 0.015\n",
    "    \n",
    "elif opt.usemodel == 'CNN_BiLSTM_CRF_MC':\n",
    "    parameters['lower'] = 1\n",
    "    parameters['zeros'] = 0\n",
    "    parameters['cpdim'] = 0\n",
    "    parameters['dpout'] = 0.5\n",
    "    parameters['chdim'] = 25\n",
    "    parameters['tgsch'] = 'iobes'\n",
    "\n",
    "    parameters['wldim'] = 200\n",
    "    parameters['cldim'] = 25\n",
    "    parameters['cnchl'] = 25\n",
    "    \n",
    "    parameters['lrate'] = 0.015\n",
    "\n",
    "elif opt.usemodel == 'CNN_CNN_LSTM':\n",
    "    parameters['lower'] = 1\n",
    "    parameters['zeros'] = 0\n",
    "    parameters['cpdim'] = 0\n",
    "    parameters['dpout'] = 0.5\n",
    "    parameters['chdim'] = 25\n",
    "    parameters['tgsch'] = 'iobes'\n",
    "    \n",
    "    parameters['w1chl'] = 400\n",
    "    parameters['w2chl'] = 400\n",
    "    parameters['cldim'] = 25\n",
    "    parameters['cnchl'] = 50\n",
    "    parameters['dchid'] = 50\n",
    "    \n",
    "    parameters['lrate'] = 0.01\n",
    "    \n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "use_dataset = opt.dataset\n",
    "dataset_path = os.path.join('datasets', use_dataset)\n",
    "result_path = os.path.join(opt.result_path, use_dataset)\n",
    "model_name = opt.usemodel\n",
    "model_load = opt.reload\n",
    "loader = Loader()\n",
    "\n",
    "print('Model:', model_name)\n",
    "print('Dataset:', use_dataset)\n",
    "\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "    \n",
    "if not os.path.exists(os.path.join(result_path,model_name)):\n",
    "    os.makedirs(os.path.join(result_path,model_name))\n",
    "\n",
    "if opt.dataset == 'conll':\n",
    "    train_data, dev_data, test_data, test_train_data, mappings = loader.load_conll(dataset_path, parameters)\n",
    "\n",
    "word_to_id = mappings['word_to_id']\n",
    "tag_to_id = mappings['tag_to_id']\n",
    "char_to_id = mappings['char_to_id']\n",
    "word_embeds = mappings['word_embeds']\n",
    "\n",
    "print('Load Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'str_words': ['Peter', 'Blackburn'],\n",
       " 'words': [792, 1895],\n",
       " 'chars': [[50, 1, 3, 1, 7], [44, 9, 2, 12, 29, 21, 13, 7, 4]],\n",
       " 'caps': [2, 2],\n",
       " 'tags': [2, 4]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-LOC',\n",
       " 2: 'B-PER',\n",
       " 3: 'B-ORG',\n",
       " 4: 'I-PER',\n",
       " 5: 'I-ORG',\n",
       " 6: 'B-MISC',\n",
       " 7: 'I-LOC',\n",
       " 8: 'I-MISC',\n",
       " 9: '<START>',\n",
       " 10: '<STOP>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mappings['id_to_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size, input_dropout_p, output_dropout_p, n_layers, rnn_cell, max_len=25):\n",
    "        super(baseRNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.input_dropout_p = input_dropout_p\n",
    "        self.output_dropout_p = output_dropout_p\n",
    "        \n",
    "        if rnn_cell.lower() == 'lstm':\n",
    "            self.rnn_cell = nn.LSTM\n",
    "        elif rnn_cell.lower() == 'gru':\n",
    "            self.rnn_cell = nn.GRU\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN Cell: {0}\".format(rnn_cell))\n",
    "\n",
    "        self.input_dropout = nn.Dropout(p=input_dropout_p)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "class CharEncoderCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size ,out_channels, kernel_width, pad_width, \n",
    "                 input_dropout_p=0, output_dropout_p=0, in_channels=1):\n",
    "        \n",
    "        super(CharEncoderCNN, self).__init__()\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        self.input_dropout = nn.Dropout(input_dropout_p)\n",
    "        self.output_dropout = nn.Dropout(output_dropout_p)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.cnn = nn.Conv2d(in_channels, out_channels, kernel_size = (kernel_width, embedding_size),\n",
    "                             padding = (pad_width,0))\n",
    "\n",
    "    def forward(self, input_var, input_lengths=None):\n",
    "        \n",
    "        embedded = self.embedding(input_var).unsqueeze(1)\n",
    "        embedded = self.input_dropout(embedded)\n",
    "        output = self.cnn(embedded)\n",
    "        output = nn.functional.max_pool2d(output, kernel_size=(output.size(2), 1))\n",
    "        output = output.squeeze(3).squeeze(2)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class DecoderCRF(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dimension, tag_to_ix, input_dropout_p=0.5):\n",
    "        \n",
    "        super(DecoderCRF, self).__init__()\n",
    "        \n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        \n",
    "        self.dropout = nn.Dropout(input_dropout_p)\n",
    "        self.hidden2tag = nn.Linear(input_dimension, self.tagset_size)\n",
    "        \n",
    "        self.transitions = nn.Parameter(torch.zeros(self.tagset_size, self.tagset_size))\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "    \n",
    "\n",
    "    def viterbi_decode(self, feats, mask ,usecuda = True, score_only= False):\n",
    "    \n",
    "        batch_size, sequence_len, num_tags = feats.size()\n",
    "        \n",
    "        assert num_tags == self.tagset_size\n",
    "        \n",
    "        mask = mask.transpose(0, 1).contiguous()\n",
    "        feats = feats.transpose(0, 1).contiguous()\n",
    "        \n",
    "        backpointers = []\n",
    "        \n",
    "        all_forward_vars = Variable(torch.Tensor(sequence_len, \n",
    "                                    batch_size, num_tags).fill_(0.)).cuda()\n",
    "        \n",
    "        init_vars = torch.Tensor(batch_size, num_tags).fill_(-10000.)\n",
    "        init_vars[:,self.tag_to_ix[START_TAG]] = 0.\n",
    "        if usecuda:\n",
    "            forward_var = Variable(init_vars).cuda()\n",
    "        else:\n",
    "            forward_var = Variable(init_vars)\n",
    "        \n",
    "        for i in range(sequence_len):\n",
    "            broadcast_forward = forward_var.view(batch_size, 1, num_tags)\n",
    "            transition_scores = self.transitions.view(1, num_tags, num_tags)\n",
    "            \n",
    "            next_tag_var = broadcast_forward + transition_scores\n",
    "            \n",
    "            viterbivars_t, bptrs_t = torch.max(next_tag_var, dim=2)\n",
    "            \n",
    "            forward_var = viterbivars_t + feats[i]\n",
    "            all_forward_vars[i,:,:] = forward_var\n",
    "\n",
    "            bptrs_t = bptrs_t.squeeze().data.cpu().numpy()\n",
    "            backpointers.append(bptrs_t)\n",
    "        \n",
    "        mask_sum = torch.sum(mask, dim = 0, keepdim =True) - 1\n",
    "        mask_sum_ex = mask_sum.view(1, batch_size, 1).expand(1, batch_size, num_tags)\n",
    "        final_forward_var = all_forward_vars.gather(0, mask_sum_ex).squeeze(0)\n",
    "        \n",
    "        terminal_var = final_forward_var + self.transitions[self.tag_to_ix[STOP_TAG]].view(1, num_tags)\n",
    "        terminal_var.data[:,self.tag_to_ix[STOP_TAG]] = -10000.\n",
    "        terminal_var.data[:,self.tag_to_ix[START_TAG]] = -10000.\n",
    "        \n",
    "        path_score, best_tag_id = torch.max(terminal_var, dim = 1)\n",
    "                \n",
    "        if score_only:\n",
    "            return path_score\n",
    "        \n",
    "        n_mask_sum = mask_sum.squeeze().data.cpu().numpy() + 1\n",
    "        best_tag_id = best_tag_id.data.cpu().numpy()\n",
    "        decoded_tags = []\n",
    "        for i in range(batch_size):\n",
    "            best_path = [best_tag_id[i]]\n",
    "            bp_list = reversed([itm[i] for itm in backpointers[:n_mask_sum[i]]])\n",
    "            for bptrs_t in bp_list:\n",
    "                best_tag_id[i] = bptrs_t[best_tag_id[i]]\n",
    "                best_path.append(best_tag_id[i])\n",
    "            start = best_path.pop()\n",
    "            assert start == self.tag_to_ix[START_TAG]\n",
    "            best_path.reverse()\n",
    "            decoded_tags.append(best_path)\n",
    "        \n",
    "        return path_score, decoded_tags\n",
    "    \n",
    "    def crf_forward(self, feats, mask, usecuda=True):\n",
    "        \n",
    "        batch_size, sequence_length, num_tags = feats.size()\n",
    "        \n",
    "        mask = mask.float().transpose(0, 1).contiguous()\n",
    "        feats = feats.transpose(0, 1).contiguous()\n",
    "        \n",
    "        init_alphas = torch.Tensor(batch_size, num_tags).fill_(-10000.)\n",
    "        init_alphas[:,self.tag_to_ix[START_TAG]] = 0.\n",
    "        if usecuda:\n",
    "            forward_var = Variable(init_alphas).cuda()\n",
    "        else:\n",
    "            forward_var = Variable(init_alphas)\n",
    "        \n",
    "        for i in range(sequence_length):\n",
    "            emit_score = feats[i].view(batch_size, num_tags, 1)\n",
    "            transition_scores = self.transitions.view(1, num_tags, num_tags)\n",
    "            broadcast_forward = forward_var.view(batch_size, 1, num_tags)\n",
    "            tag_var = broadcast_forward + transition_scores + emit_score \n",
    "            \n",
    "            forward_var = (log_sum_exp(tag_var, dim = 2) * mask[i].view(batch_size, 1) +\n",
    "                            forward_var * (1 - mask[i]).view(batch_size, 1))\n",
    "            \n",
    "        terminal_var = (forward_var + (self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1))\n",
    "        alpha = log_sum_exp(terminal_var, dim = 1)\n",
    "        \n",
    "        return alpha\n",
    "        \n",
    "    \n",
    "    def score_sentence(self, feats, tags, mask, usecuda=True):\n",
    "                \n",
    "        batch_size, sequence_length, num_tags = feats.size()\n",
    "        \n",
    "        feats = feats.transpose(0, 1).contiguous()\n",
    "        tags = tags.transpose(0, 1).contiguous()\n",
    "        mask = mask.float().transpose(0, 1).contiguous()\n",
    "                \n",
    "        broadcast_transitions = self.transitions.view(1, num_tags, num_tags).expand(batch_size, num_tags, num_tags)\n",
    "        \n",
    "        score = self.transitions[:,self.tag_to_ix[START_TAG]].index_select(0, tags[0])\n",
    "        \n",
    "        for i in range(sequence_length - 1):\n",
    "            current_tag, next_tag = tags[i], tags[i+1]\n",
    "            \n",
    "            transition_score = (\n",
    "                     broadcast_transitions\n",
    "                    .gather(1, next_tag.view(batch_size, 1, 1).expand(batch_size, 1, num_tags))\n",
    "                    .squeeze(1)\n",
    "                    .gather(1, current_tag.view(batch_size, 1))\n",
    "                    .squeeze(1)\n",
    "                    )\n",
    "\n",
    "            emit_score = feats[i].gather(1, current_tag.view(batch_size, 1)).squeeze(1)\n",
    "\n",
    "            score = score + transition_score* mask[i + 1] + emit_score * mask[i]  \n",
    "        last_tag_index = mask.sum(0).long() - 1\n",
    "\n",
    "        last_tags = tags.gather(0, last_tag_index.view(1, batch_size).expand(sequence_length, batch_size))\n",
    "        last_tags = last_tags[0]\n",
    "\n",
    "        last_transition_score = self.transitions[self.tag_to_ix[STOP_TAG]].index_select(0, last_tags)\n",
    "        \n",
    "        last_inputs = feats[-1]                                     \n",
    "        last_input_score = last_inputs.gather(1, last_tags.view(batch_size, 1))\n",
    "        last_input_score = last_input_score.squeeze(1)\n",
    "        \n",
    "        score = score + last_transition_score + last_input_score * mask[-1]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def decode(self, input_var, mask, usecuda=True):\n",
    "        \n",
    "        input_var = self.dropout(input_var)\n",
    "        features = self.hidden2tag(input_var)\n",
    "        score, tag_seq = self.viterbi_decode(features, mask, usecuda=usecuda)\n",
    "        \n",
    "        return score, tag_seq\n",
    "    \n",
    "    def forward(self, input_var, tags, mask=None, usecuda=True):\n",
    "        \n",
    "        if mask is None:\n",
    "            mask = torch.autograd.Variable(torch.ones(*tags.size()).long())\n",
    "        \n",
    "        input_var = self.dropout(input_var)\n",
    "        features = self.hidden2tag(input_var)\n",
    "        forward_score = self.crf_forward(features, mask, usecuda=usecuda)\n",
    "        ground_score = self.score_sentence(features, tags, mask, usecuda=usecuda)\n",
    "        \n",
    "        return forward_score-ground_score\n",
    "\n",
    "\n",
    "class WordEncoderRNN(baseRNN):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size ,hidden_size, char_size, cap_size, input_dropout_p=0.5, \n",
    "                 output_dropout_p=0, n_layers=1, bidirectional=True, rnn_cell='lstm'):\n",
    "        \n",
    "        super(WordEncoderRNN, self).__init__(vocab_size, hidden_size, input_dropout_p, \n",
    "                                             output_dropout_p, n_layers, rnn_cell)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        augmented_embedding_size = embedding_size + char_size + cap_size\n",
    "        self.rnn = self.rnn_cell(augmented_embedding_size, hidden_size, n_layers,\n",
    "                                 bidirectional=bidirectional, dropout=output_dropout_p,\n",
    "                                 batch_first=True)\n",
    "\n",
    "    def forward(self, words, char_embedding, cap_embedding, input_lengths):\n",
    "        \n",
    "        embedded = self.embedding(words)\n",
    "        if cap_embedding is not None:\n",
    "            embedded = torch.cat((embedded,char_embedding,cap_embedding),2)  \n",
    "        else:\n",
    "            embedded = torch.cat((embedded,char_embedding),2)\n",
    "    \n",
    "        embedded = self.input_dropout(embedded)\n",
    "        embedded = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first= True)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first= True)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class WordEncoderCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, char_size, kernel_width = 5, pad_width = 4, \n",
    "                 in_channels=1, out1_channels=800, out2_channels=800, cap_size=0, input_dropout_p=0.5, \n",
    "                 output_dropout_p=0):\n",
    "        \n",
    "        super(WordEncoderCNN, self).__init__()\n",
    "        \n",
    "        self.kernel_width = kernel_width\n",
    "        self.out2_channels = out2_channels\n",
    "        self.input_dropout = nn.Dropout(p=input_dropout_p)\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        new_embedding_size = embedding_size + char_size\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out1_channels, kernel_size=(kernel_width, new_embedding_size),\n",
    "                             padding = (pad_width,0))\n",
    "        self.cnn2 = nn.Conv2d(out1_channels, out2_channels, kernel_size=(kernel_width, 1),\n",
    "                             padding = (pad_width,0))\n",
    "\n",
    "    def forward(self, words, char_embedding, cap_embedding=None ,input_lengths=None):\n",
    "        \n",
    "        embedded = self.embedding(words)\n",
    "        \n",
    "        if cap_embedding:\n",
    "            embedded = torch.cat((embedded,char_embedding,cap_embedding),2)  \n",
    "        else:\n",
    "            embedded = torch.cat((embedded,char_embedding),2)\n",
    "        \n",
    "        embedded1 = embedded.unsqueeze(1)\n",
    "        embedded1 = self.input_dropout(embedded1)\n",
    "                        \n",
    "        output1 = self.cnn1(embedded1)\n",
    "        output1 = nn.functional.max_pool2d(output1, kernel_size=(self.kernel_width, 1), stride = 1)\n",
    "        \n",
    "        output2 = self.cnn2(output1)\n",
    "        output2 = nn.functional.max_pool2d(output2, kernel_size=(self.kernel_width, 1), stride = 1)\n",
    "        output2 = output2.squeeze(3).transpose(1,2)\n",
    "        \n",
    "        return output2, embedded\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size ,hidden_size, tag_size, tag_to_ix, input_dropout_p=0.5, \n",
    "                 output_dropout_p=0, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.input_dropout_p = input_dropout_p\n",
    "        self.output_dropout_p = output_dropout_p\n",
    "        \n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        \n",
    "        self.dropout = nn.Dropout(input_dropout_p)\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size + tag_size, hidden_size, n_layers, bidirectional=False)\n",
    "        self.linear = nn.Linear(hidden_size, tag_size)\n",
    "        self.ignore = -1\n",
    "        self.lossfunc = nn.CrossEntropyLoss(ignore_index= self.ignore)\n",
    "        \n",
    "    def forward_step(self, input_var, prev_tag, hidden ,usecuda=True):\n",
    "        \n",
    "        prev_tag_onehot = torch.eye(self.tagset_size)\n",
    "        prev_tag_onehot = prev_tag_onehot.index_select(0,torch.LongTensor(prev_tag))\n",
    "        \n",
    "        if usecuda:\n",
    "            prev_tag_onehot = Variable(prev_tag_onehot).cuda()\n",
    "        else:\n",
    "            prev_tag_onehot = Variable(prev_tag_onehot)\n",
    "        \n",
    "        decoder_input = torch.cat([input_var, prev_tag_onehot],1).unsqueeze(0)\n",
    "        output, hidden = self.rnn(decoder_input, hidden)\n",
    "        output = self.linear(output.squeeze(0))\n",
    "        output_tag = output.max(1)[1].data.cpu().numpy().tolist()\n",
    "\n",
    "        return output, output_tag, hidden\n",
    "        \n",
    "    def forward(self, input_var, tags, mask, usecuda=True):\n",
    "        \n",
    "        batch_size, sequence_len, _ = input_var.size()\n",
    "        \n",
    "        input_var = self.dropout(input_var)\n",
    "        \n",
    "        input_var = input_var.transpose(0, 1).contiguous()\n",
    "        \n",
    "        tags = tags.transpose(0, 1).contiguous()\n",
    "        mask = mask.float().transpose(0, 1).contiguous()\n",
    "        \n",
    "        maskedtags = tags.clone()\n",
    "        maskedtags[mask==0] = -1\n",
    "        \n",
    "        loss = 0.0\n",
    "        prev_tag = [self.tag_to_ix[START_TAG]]*batch_size\n",
    "        hidden = None\n",
    "        \n",
    "        for i in range(sequence_len):\n",
    "            output, prev_tag, hidden=self.forward_step(input_var[i], prev_tag, hidden, \n",
    "                                                       usecuda=usecuda)\n",
    "            loss += self.lossfunc(output, maskedtags[i])\n",
    "        return loss\n",
    "    \n",
    "    def decode(self, input_var, wordslen, usecuda=True):\n",
    "        \n",
    "        batch_size, sequence_len, _ = input_var.size()\n",
    "        \n",
    "        input_var = self.dropout(input_var)\n",
    "        input_var = input_var.transpose(0, 1).contiguous()\n",
    "        \n",
    "        loss = 0.0\n",
    "        prev_tag = [self.tag_to_ix[START_TAG]]*batch_size\n",
    "        hidden = None\n",
    "        \n",
    "        tag_seq = []\n",
    "        probs= []\n",
    "        for i in range(sequence_len):\n",
    "            output, prev_tag, hidden=self.forward_step(input_var[i], prev_tag, hidden, \n",
    "                                                       usecuda=usecuda)\n",
    "            tag_seq.append(prev_tag)\n",
    "            pb = nn.functional.softmax(output, dim = 1).data.cpu().numpy()\n",
    "            probs.append(pb)\n",
    "        \n",
    "        probs = np.array(probs).transpose(1,0,2)\n",
    "        \n",
    "        tag_seq = np.array(tag_seq).transpose().tolist()\n",
    "        tag_seq = [ts[:wordslen[i]] for i,ts in enumerate(tag_seq)]\n",
    "        \n",
    "        return probs, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_CNN_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, word_vocab_size, word_embedding_dim, word_out1_channels, word_out2_channels,\n",
    "                 char_vocab_size, char_embedding_dim, char_out_channels, decoder_hidden_units,\n",
    "                 tag_to_id, cap_input_dim=4, cap_embedding_dim=0, pretrained=None):\n",
    "        \n",
    "        super(CNN_CNN_LSTM, self).__init__()\n",
    "        \n",
    "        self.word_vocab_size = word_vocab_size\n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        self.word_out1_channels = word_out1_channels\n",
    "        self.word_out2_channels = word_out2_channels\n",
    "        \n",
    "        self.char_vocab_size = char_vocab_size\n",
    "        self.char_embedding_dim = char_embedding_dim\n",
    "        self.char_out_channels = char_out_channels\n",
    "        \n",
    "        self.cap_input_dim = cap_input_dim\n",
    "        self.cap_embedding_dim = cap_embedding_dim\n",
    "        \n",
    "        self.tag_to_ix = tag_to_id\n",
    "        self.tagset_size = len(tag_to_id)\n",
    "        \n",
    "        self.initializer = Initializer()\n",
    "        self.loader = Loader()\n",
    "        \n",
    "        if self.cap_input_dim and self.cap_embedding_dim:\n",
    "            self.cap_embedder = nn.Embedding(self.cap_input_dim, self.cap_embedding_dim)\n",
    "            self.initializer.init_embedding(self.cap_embedder.weight)\n",
    "        \n",
    "        self.char_encoder = CharEncoderCNN(char_vocab_size, char_embedding_dim, char_out_channels, \n",
    "                                           kernel_width=3, pad_width=1)\n",
    "        \n",
    "        self.initializer.init_embedding(self.char_encoder.embedding.weight)\n",
    "        \n",
    "        self.word_encoder = WordEncoderCNN(word_vocab_size, word_embedding_dim, char_out_channels,\n",
    "                                           kernel_width = 3, pad_width = 2, input_dropout_p=0.5,\n",
    "                                           out1_channels=word_out1_channels, out2_channels=word_out2_channels)\n",
    "        \n",
    "        if pretrained is not None:\n",
    "            self.word_encoder.embedding.weight = nn.Parameter(torch.FloatTensor(pretrained))\n",
    "        \n",
    "        augmented_decoder_inp_size = (word_out2_channels + word_embedding_dim + \n",
    "                                      char_out_channels + cap_embedding_dim)\n",
    "        self.decoder = DecoderRNN(augmented_decoder_inp_size, decoder_hidden_units, self.tagset_size, \n",
    "                                  self.tag_to_ix, input_dropout_p=0.5)\n",
    "        \n",
    "    def forward(self, words, tags, chars, caps, wordslen, charslen, tagsmask, usecuda=True):\n",
    "        \n",
    "        batch_size, max_len = words.size()\n",
    "        \n",
    "        cap_features = self.cap_embedder(caps) if self.cap_embedding_dim else None\n",
    "        \n",
    "        char_features = self.char_encoder(chars)\n",
    "        char_features = char_features.view(batch_size, max_len, -1)\n",
    "        \n",
    "        word_features, word_input_feats = self.word_encoder(words, char_features, cap_features)\n",
    "        \n",
    "        new_word_features = torch.cat((word_features,word_input_feats),2)\n",
    "        loss = self.decoder(new_word_features, tags, tagsmask, usecuda=usecuda)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def decode(self, words, chars, caps, wordslen, charslen, tagsmask, usecuda=True, \n",
    "               score_only = False):\n",
    "        \n",
    "        batch_size, max_len = words.size()\n",
    "        \n",
    "        cap_features = self.cap_embedder(caps) if self.cap_embedding_dim else None\n",
    "        \n",
    "        char_features = self.char_encoder(chars)\n",
    "        char_features = char_features.view(batch_size, max_len, -1)\n",
    "        \n",
    "        word_features, word_input_feats = self.word_encoder(words, char_features, cap_features)\n",
    "        \n",
    "        new_word_features = torch.cat((word_features,word_input_feats),2)\n",
    "        \n",
    "        if score_only:\n",
    "            score, _ = self.decoder.decode(new_word_features, wordslen, usecuda=usecuda)\n",
    "            return score\n",
    "        \n",
    "        score, tag_seq = self.decoder.decode(new_word_features, wordslen, usecuda=usecuda)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model............................................................................\n",
      "CNN_BiLSTM_CRF\n",
      "Initial learning rate is: 0.015\n"
     ]
    }
   ],
   "source": [
    "if model_load:\n",
    "    print ('Loading Saved Weights....................................................................')\n",
    "    model_path = os.path.join(result_path, model_name, opt.checkpoint, 'modelweights')\n",
    "    model = torch.load(model_path)\n",
    "else:\n",
    "    print('Building Model............................................................................')\n",
    "    if (model_name == 'CNN_BiLSTM_CRF'):\n",
    "        print ('CNN_BiLSTM_CRF')\n",
    "        word_vocab_size = len(word_to_id)\n",
    "        word_embedding_dim = parameters['wrdim']\n",
    "        word_hidden_dim = parameters['wldim']\n",
    "        char_vocab_size = len(char_to_id)\n",
    "        char_embedding_dim = parameters['chdim']\n",
    "        char_out_channels = parameters['cnchl']\n",
    "\n",
    "        model = CNN_BiLSTM_CRF(word_vocab_size, word_embedding_dim, word_hidden_dim, char_vocab_size,\n",
    "                               char_embedding_dim, char_out_channels, tag_to_id, pretrained = word_embeds,\n",
    "                               cap_embedding_dim = 10)\n",
    "        \n",
    "    elif (model_name == 'CNN_BiLSTM_CRF_MC'):\n",
    "        print ('CNN_BiLSTM_CRF_MC')\n",
    "        word_vocab_size = len(word_to_id)\n",
    "        word_embedding_dim = parameters['wrdim']\n",
    "        word_hidden_dim = parameters['wldim']\n",
    "        char_vocab_size = len(char_to_id)\n",
    "        char_embedding_dim = parameters['chdim']\n",
    "        char_out_channels = parameters['cnchl']\n",
    "\n",
    "        model = CNN_BiLSTM_CRF_MC(word_vocab_size, word_embedding_dim, word_hidden_dim, char_vocab_size,\n",
    "                               char_embedding_dim, char_out_channels, tag_to_id, pretrained = word_embeds)\n",
    "\n",
    "    elif (model_name == 'CNN_CNN_LSTM'):\n",
    "        print ('CNN_CNN_LSTM')\n",
    "        word_vocab_size = len(word_to_id)\n",
    "        word_embedding_dim = parameters['wrdim']\n",
    "        word_out1_channels = parameters['w1chl']\n",
    "        word_out2_channels = parameters['w2chl']\n",
    "        char_vocab_size = len(char_to_id)\n",
    "        char_embedding_dim = parameters['chdim']\n",
    "        char_out_channels = parameters['cnchl']\n",
    "        decoder_hidden_units = parameters['dchid']\n",
    "\n",
    "        model = CNN_CNN_LSTM(word_vocab_size, word_embedding_dim, word_out1_channels, word_out2_channels,\n",
    "                             char_vocab_size, char_embedding_dim, char_out_channels, decoder_hidden_units,\n",
    "                             tag_to_id, pretrained = word_embeds)\n",
    "    \n",
    "    \n",
    "model.cuda()\n",
    "learning_rate = parameters['lrate']\n",
    "print('Initial learning rate is: %s' %(learning_rate))\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self, result_path, model_name, mappings, usecuda=True):\n",
    "        self.result_path = result_path\n",
    "        self.model_name = model_name\n",
    "        self.tag_to_id = mappings['tag_to_id']\n",
    "        self.id_to_tag = mappings['id_to_tag']\n",
    "        self.usecuda = usecuda\n",
    "\n",
    "    def evaluate_conll(self, model, dataset, best_F, eval_script='./datasets/conll/conlleval',\n",
    "                       checkpoint_folder='.', record_confmat = False, batch_size = 32):\n",
    "        \n",
    "        prediction = []\n",
    "        save = False\n",
    "        new_F = 0.0\n",
    "        confusion_matrix = torch.zeros((len(self.tag_to_id) - 2, len(self.tag_to_id) - 2))\n",
    "    \n",
    "        data_batches = create_batches(dataset, batch_size = batch_size, str_words = True,\n",
    "                                      tag_padded = False)\n",
    "\n",
    "        for data in data_batches:\n",
    "\n",
    "            words = data['words']\n",
    "            chars = data['chars']\n",
    "            caps = data['caps']\n",
    "            mask = data['tagsmask']\n",
    "\n",
    "            if self.usecuda:\n",
    "                words = Variable(torch.LongTensor(words)).cuda()\n",
    "                chars = Variable(torch.LongTensor(chars)).cuda()\n",
    "                caps = Variable(torch.LongTensor(caps)).cuda()\n",
    "                mask = Variable(torch.LongTensor(mask)).cuda()\n",
    "            else:\n",
    "                words = Variable(torch.LongTensor(words))\n",
    "                chars = Variable(torch.LongTensor(chars))\n",
    "                caps = Variable(torch.LongTensor(caps))\n",
    "                mask = Variable(torch.LongTensor(mask))\n",
    "\n",
    "            wordslen = data['wordslen']\n",
    "            charslen = data['charslen']\n",
    "            \n",
    "            str_words = data['str_words']\n",
    "            \n",
    "            _, out = model.decode(words, chars, caps, wordslen, charslen, mask, usecuda = self.usecuda)\n",
    "#             print (out)\n",
    "#             assert False\n",
    "            \n",
    "            ground_truth_id = data['tags']\n",
    "            predicted_id = out            \n",
    "            \n",
    "            for (swords, sground_truth_id, spredicted_id) in zip(str_words, ground_truth_id, predicted_id):\n",
    "                for (word, true_id, pred_id) in zip(swords, sground_truth_id, spredicted_id):\n",
    "                    line = ' '.join([word, self.id_to_tag[true_id], self.id_to_tag[pred_id]])\n",
    "                    prediction.append(line)\n",
    "                    confusion_matrix[true_id, pred_id] += 1\n",
    "                prediction.append('')\n",
    "\n",
    "        predf = os.path.join(self.result_path, self.model_name, checkpoint_folder ,'pred.txt')\n",
    "        scoref = os.path.join(self.result_path, self.model_name, checkpoint_folder ,'score.txt')\n",
    "\n",
    "        with open(predf, 'w') as f:\n",
    "            f.write('\\n'.join(prediction))\n",
    "\n",
    "        os.system('%s < %s > %s' % (eval_script, predf, scoref))\n",
    "\n",
    "        eval_lines = [l.rstrip() for l in codecs.open(scoref, 'r', 'utf8')]\n",
    "\n",
    "        for i, line in enumerate(eval_lines):\n",
    "            print(line)\n",
    "            if i == 1:\n",
    "                new_F = float(line.strip().split()[-1])\n",
    "                if new_F > best_F:\n",
    "                    best_F = new_F\n",
    "                    save = True\n",
    "                    print('the best F is ', new_F)\n",
    "        if record_confmat:\n",
    "            print((\"{: >2}{: >7}{: >7}%s{: >9}\" % (\"{: >7}\" * confusion_matrix.size(0))).format(\n",
    "                \"ID\", \"NE\", \"Total\",\n",
    "                *([self.id_to_tag[i] for i in range(confusion_matrix.size(0))] + [\"Percent\"])\n",
    "            ))\n",
    "            for i in range(confusion_matrix.size(0)):\n",
    "                print((\"{: >2}{: >7}{: >7}%s{: >9}\" % (\"{: >7}\" * confusion_matrix.size(0))).format(\n",
    "                    str(i), self.id_to_tag[i], str(confusion_matrix[i].sum()),\n",
    "                    *([confusion_matrix[i][j] for j in range(confusion_matrix.size(0))] +\n",
    "                      [\"%.3f\" % (confusion_matrix[i][i] * 100. / max(1, confusion_matrix[i].sum()))])\n",
    "                ))\n",
    "            \n",
    "        return best_F, new_F, save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class Trainer(object):\n",
    "    \n",
    "    def __init__(self, model, optimizer, result_path, model_name, usedataset, mappings, \n",
    "                 eval_every=1, usecuda = True):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.eval_every = eval_every\n",
    "        self.model_name = os.path.join(result_path, model_name)\n",
    "        self.usecuda = usecuda\n",
    "        \n",
    "        if usedataset=='conll':\n",
    "            self.evaluator = Evaluator(result_path, model_name, mappings, usecuda).evaluate_conll\n",
    "    \n",
    "    def adjust_learning_rate(self, optimizer, lr):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "            \n",
    "    def train_model(self, num_epochs, train_data, dev_data, test_train_data, test_data, learning_rate,\n",
    "                    checkpoint_folder='.', eval_test_train=True, plot_every=1, adjust_lr=True,\n",
    "                    batch_size = 2):\n",
    "        \n",
    "        losses = []\n",
    "        loss = 0.0\n",
    "        best_dev_F = -1.0\n",
    "        best_test_F = -1.0\n",
    "        best_train_F = -1.0\n",
    "        all_F=[[0,0,0]]\n",
    "        count = 0\n",
    "        word_count = 0\n",
    "        \n",
    "        self.model.train(True)\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            t=time.time()\n",
    "            \n",
    "            #Random Batching Ensure\n",
    "            train_batches = create_batches(train_data, batch_size= batch_size)\n",
    "            #Random Permutation instead of Range.\n",
    "            for i, index in enumerate(np.arange(len(train_batches[:5]))):\n",
    "                \n",
    "                data = train_batches[index]\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                words = data['words']\n",
    "                tags = data['tags']\n",
    "                chars = data['chars']\n",
    "                caps = data['caps']\n",
    "                mask = data['tagsmask']\n",
    "                \n",
    "                if self.usecuda:\n",
    "                    words = Variable(torch.LongTensor(words)).cuda()\n",
    "                    chars = Variable(torch.LongTensor(chars)).cuda()\n",
    "                    caps = Variable(torch.LongTensor(caps)).cuda()\n",
    "                    mask = Variable(torch.LongTensor(mask)).cuda()\n",
    "                    tags = Variable(torch.LongTensor(tags)).cuda()\n",
    "                else:\n",
    "                    words = Variable(torch.LongTensor(words))\n",
    "                    chars = Variable(torch.LongTensor(chars))\n",
    "                    caps = Variable(torch.LongTensor(caps))\n",
    "                    mask = Variable(torch.LongTensor(mask))\n",
    "                    tags = Variable(torch.LongTensor(tags))\n",
    "                \n",
    "                wordslen = data['wordslen']\n",
    "                charslen = data['charslen']\n",
    "                batch_score = self.model(words, tags, chars, caps, wordslen, charslen, mask,\n",
    "                                         usecuda=self.usecuda)\n",
    "                loss += np.mean(batch_score.data.cpu().numpy()/np.array(data['wordslen']))\n",
    "                score = torch.sum(batch_score)\n",
    "                score.backward()\n",
    "                \n",
    "                \n",
    "                nn.utils.clip_grad_norm(self.model.parameters(), 5.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                count += 1\n",
    "                word_count += len(data['words'])\n",
    "                \n",
    "                if count % plot_every == 0:\n",
    "                    loss /= plot_every\n",
    "                    print(word_count, ': ', loss)\n",
    "                    if losses == []:\n",
    "                        losses.append(loss)\n",
    "                    losses.append(loss)\n",
    "                    loss = 0.0\n",
    "                    \n",
    "            if adjust_lr:\n",
    "                self.adjust_learning_rate(self.optimizer, lr=learning_rate/(1+0.05*count/len(train_data)))\n",
    "            \n",
    "            if epoch%self.eval_every==0:\n",
    "                \n",
    "                self.model.train(False)\n",
    "                \n",
    "                if eval_test_train:\n",
    "                    best_train_F, new_train_F, _ = self.evaluator(self.model, test_train_data, best_train_F, \n",
    "                                                                  checkpoint_folder=checkpoint_folder)\n",
    "                else:\n",
    "                    best_train_F, new_train_F, _ = 0, 0, 0\n",
    "                best_dev_F, new_dev_F, save = self.evaluator(self.model, dev_data, best_dev_F,\n",
    "                                                             checkpoint_folder=checkpoint_folder)\n",
    "                if save:\n",
    "                    torch.save(self.model, os.path.join(self.model_name, checkpoint_folder, 'modelweights'))\n",
    "                best_test_F, new_test_F, _ = self.evaluator(self.model, test_data, best_test_F,\n",
    "                                                            checkpoint_folder=checkpoint_folder)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "                all_F.append([new_train_F, new_dev_F, new_test_F])\n",
    "                \n",
    "                self.model.train(True)\n",
    "\n",
    "            print('*'*80)\n",
    "            print('Epoch %d Complete: Time Taken %d' %(epoch ,time.time() - t))\n",
    "\n",
    "        return losses, all_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 :  7.767260127597385\n",
      "4 :  17.02818094889323\n",
      "6 :  3.244954904386608\n",
      "8 :  1.5622842884063721\n",
      "10 :  1.5611279056799459\n",
      "processed 50145 tokens with 6019 phrases; found: 120 phrases; correct: 1.\n",
      "accuracy:  82.38%; precision:   0.83%; recall:   0.02%; FB1:   0.03\n",
      "the best F is  0.03\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  82\n",
      "              ORG: precision:   3.03%; recall:   0.08%; FB1:   0.16  33\n",
      "              PER: precision:   0.00%; recall:   0.00%; FB1:   0.00  5\n",
      "processed 51362 tokens with 5942 phrases; found: 100 phrases; correct: 0.\n",
      "accuracy:  83.16%; precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
      "the best F is  0.0\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  4\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  68\n",
      "              ORG: precision:   0.00%; recall:   0.00%; FB1:   0.00  21\n",
      "              PER: precision:   0.00%; recall:   0.00%; FB1:   0.00  7\n",
      "processed 46435 tokens with 5648 phrases; found: 252 phrases; correct: 3.\n",
      "accuracy:  82.18%; precision:   1.19%; recall:   0.05%; FB1:   0.10\n",
      "the best F is  0.1\n",
      "              LOC: precision:  10.00%; recall:   0.06%; FB1:   0.12  10\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  137\n",
      "              ORG: precision:   1.85%; recall:   0.06%; FB1:   0.12  54\n",
      "              PER: precision:   1.96%; recall:   0.06%; FB1:   0.12  51\n",
      "********************************************************************************\n",
      "Epoch 1 Complete: Time Taken 18\n",
      "12 :  4.923642476399739\n",
      "14 :  7.6762644449869795\n",
      "16 :  2.1712015250794234\n",
      "18 :  1.0511688995361328\n",
      "20 :  1.9293209760805814\n",
      "processed 50145 tokens with 6019 phrases; found: 149 phrases; correct: 0.\n",
      "accuracy:  82.38%; precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  119\n",
      "              ORG: precision:   0.00%; recall:   0.00%; FB1:   0.00  25\n",
      "              PER: precision:   0.00%; recall:   0.00%; FB1:   0.00  4\n",
      "processed 51362 tokens with 5942 phrases; found: 145 phrases; correct: 1.\n",
      "accuracy:  83.17%; precision:   0.69%; recall:   0.02%; FB1:   0.03\n",
      "the best F is  0.03\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  5\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  128\n",
      "              ORG: precision:   0.00%; recall:   0.00%; FB1:   0.00  6\n",
      "              PER: precision:  16.67%; recall:   0.05%; FB1:   0.11  6\n",
      "processed 46435 tokens with 5648 phrases; found: 260 phrases; correct: 4.\n",
      "accuracy:  82.24%; precision:   1.54%; recall:   0.07%; FB1:   0.14\n",
      "the best F is  0.14\n",
      "              LOC: precision:  12.50%; recall:   0.06%; FB1:   0.12  8\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  190\n",
      "              ORG: precision:   0.00%; recall:   0.00%; FB1:   0.00  19\n",
      "              PER: precision:   6.98%; recall:   0.19%; FB1:   0.36  43\n",
      "********************************************************************************\n",
      "Epoch 2 Complete: Time Taken 18\n",
      "22 :  3.995361751980252\n",
      "24 :  5.520550537109375\n",
      "26 :  1.8548675865255375\n",
      "28 :  1.3757131576538086\n",
      "30 :  1.7483311822515657\n",
      "processed 50145 tokens with 6019 phrases; found: 1021 phrases; correct: 24.\n",
      "accuracy:  81.88%; precision:   2.35%; recall:   0.40%; FB1:   0.68\n",
      "the best F is  0.68\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "             MISC: precision:   0.49%; recall:   0.43%; FB1:   0.46  811\n",
      "              ORG: precision:   9.69%; recall:   1.56%; FB1:   2.69  196\n",
      "              PER: precision:   9.09%; recall:   0.06%; FB1:   0.11  11\n",
      "processed 51362 tokens with 5942 phrases; found: 930 phrases; correct: 17.\n",
      "accuracy:  82.80%; precision:   1.83%; recall:   0.29%; FB1:   0.49\n",
      "the best F is  0.49\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  4\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  727\n",
      "              ORG: precision:   8.29%; recall:   1.12%; FB1:   1.97  181\n",
      "              PER: precision:  11.11%; recall:   0.11%; FB1:   0.22  18\n",
      "processed 46435 tokens with 5648 phrases; found: 1313 phrases; correct: 25.\n",
      "accuracy:  81.64%; precision:   1.90%; recall:   0.44%; FB1:   0.72\n",
      "the best F is  0.72\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  5\n",
      "             MISC: precision:   0.20%; recall:   0.28%; FB1:   0.23  1018\n",
      "              ORG: precision:   7.73%; recall:   1.02%; FB1:   1.81  220\n",
      "              PER: precision:   8.57%; recall:   0.37%; FB1:   0.71  70\n",
      "********************************************************************************\n",
      "Epoch 3 Complete: Time Taken 19\n",
      "32 :  3.9013110796610513\n",
      "34 :  4.969096374511719\n",
      "36 :  1.7317567528867301\n",
      "38 :  0.6298526382446289\n",
      "40 :  1.3571389393456652\n",
      "processed 50145 tokens with 6019 phrases; found: 419 phrases; correct: 14.\n",
      "accuracy:  82.22%; precision:   3.34%; recall:   0.23%; FB1:   0.43\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  163\n",
      "              ORG: precision:   4.93%; recall:   0.90%; FB1:   1.53  223\n",
      "              PER: precision:  10.00%; recall:   0.17%; FB1:   0.33  30\n",
      "processed 51362 tokens with 5942 phrases; found: 360 phrases; correct: 17.\n",
      "accuracy:  83.08%; precision:   4.72%; recall:   0.29%; FB1:   0.54\n",
      "the best F is  0.54\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  2\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  155\n",
      "              ORG: precision:   5.33%; recall:   0.67%; FB1:   1.19  169\n",
      "              PER: precision:  23.53%; recall:   0.43%; FB1:   0.85  34\n",
      "processed 46435 tokens with 5648 phrases; found: 563 phrases; correct: 22.\n",
      "accuracy:  82.11%; precision:   3.91%; recall:   0.39%; FB1:   0.71\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "             MISC: precision:   0.00%; recall:   0.00%; FB1:   0.00  225\n",
      "              ORG: precision:   3.02%; recall:   0.42%; FB1:   0.74  232\n",
      "              PER: precision:  14.56%; recall:   0.93%; FB1:   1.74  103\n",
      "********************************************************************************\n",
      "Epoch 4 Complete: Time Taken 19\n",
      "42 :  3.2521835698021784\n",
      "44 :  4.995113118489583\n",
      "46 :  1.2912301728219697\n",
      "48 :  0.7541707420349122\n",
      "50 :  1.0481557330569706\n",
      "processed 50145 tokens with 6019 phrases; found: 859 phrases; correct: 39.\n",
      "accuracy:  82.13%; precision:   4.54%; recall:   0.65%; FB1:   1.13\n",
      "the best F is  1.13\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "             MISC: precision:   0.57%; recall:   0.32%; FB1:   0.41  530\n",
      "              ORG: precision:   8.33%; recall:   1.81%; FB1:   2.97  264\n",
      "              PER: precision:  22.58%; recall:   0.79%; FB1:   1.53  62\n",
      "processed 51362 tokens with 5942 phrases; found: 739 phrases; correct: 36.\n",
      "accuracy:  83.00%; precision:   4.87%; recall:   0.61%; FB1:   1.08\n",
      "the best F is  1.08\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
      "             MISC: precision:   0.22%; recall:   0.11%; FB1:   0.15  450\n",
      "              ORG: precision:   6.16%; recall:   0.97%; FB1:   1.68  211\n",
      "              PER: precision:  28.57%; recall:   1.19%; FB1:   2.29  77\n",
      "processed 46435 tokens with 5648 phrases; found: 1021 phrases; correct: 47.\n",
      "accuracy:  82.07%; precision:   4.60%; recall:   0.83%; FB1:   1.41\n",
      "the best F is  1.41\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "             MISC: precision:   0.35%; recall:   0.28%; FB1:   0.31  574\n",
      "              ORG: precision:   6.19%; recall:   1.14%; FB1:   1.93  307\n",
      "              PER: precision:  18.98%; recall:   1.61%; FB1:   2.96  137\n",
      "********************************************************************************\n",
      "Epoch 5 Complete: Time Taken 19\n",
      "52 :  2.3628872765435114\n",
      "54 :  5.000640869140625\n",
      "56 :  1.1672981388990713\n",
      "58 :  0.5615359878540039\n",
      "60 :  0.970108683965381\n",
      "processed 50145 tokens with 6019 phrases; found: 1394 phrases; correct: 59.\n",
      "accuracy:  82.04%; precision:   4.23%; recall:   0.98%; FB1:   1.59\n",
      "the best F is  1.59\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "             MISC: precision:   1.81%; recall:   2.34%; FB1:   2.04  1214\n",
      "              ORG: precision:   9.46%; recall:   0.58%; FB1:   1.09  74\n",
      "              PER: precision:  29.13%; recall:   1.69%; FB1:   3.20  103\n",
      "processed 51362 tokens with 5942 phrases; found: 1194 phrases; correct: 59.\n",
      "accuracy:  82.95%; precision:   4.94%; recall:   0.99%; FB1:   1.65\n",
      "the best F is  1.65\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  1\n",
      "             MISC: precision:   2.07%; recall:   2.28%; FB1:   2.17  1015\n",
      "              ORG: precision:   5.33%; recall:   0.30%; FB1:   0.56  75\n",
      "              PER: precision:  33.01%; recall:   1.85%; FB1:   3.50  103\n",
      "processed 46435 tokens with 5648 phrases; found: 1461 phrases; correct: 83.\n",
      "accuracy:  82.02%; precision:   5.68%; recall:   1.47%; FB1:   2.34\n",
      "the best F is  2.34\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  3\n",
      "             MISC: precision:   2.62%; recall:   4.42%; FB1:   3.29  1183\n",
      "              ORG: precision:   6.45%; recall:   0.36%; FB1:   0.68  93\n",
      "              PER: precision:  25.27%; recall:   2.84%; FB1:   5.11  182\n",
      "********************************************************************************\n",
      "Epoch 6 Complete: Time Taken 19\n",
      "62 :  1.888281848695543\n",
      "64 :  3.5845921834309897\n",
      "66 :  1.117954287710777\n",
      "68 :  0.4611145782470703\n",
      "70 :  0.7227330410342419\n",
      "processed 50145 tokens with 6019 phrases; found: 998 phrases; correct: 84.\n",
      "accuracy:  82.42%; precision:   8.42%; recall:   1.40%; FB1:   2.39\n",
      "the best F is  2.39\n",
      "              LOC: precision:  25.00%; recall:   0.10%; FB1:   0.19  8\n",
      "             MISC: precision:   1.25%; recall:   0.64%; FB1:   0.85  480\n",
      "              ORG: precision:   6.13%; recall:   1.32%; FB1:   2.17  261\n",
      "              PER: precision:  24.10%; recall:   3.38%; FB1:   5.93  249\n",
      "processed 51362 tokens with 5942 phrases; found: 864 phrases; correct: 63.\n",
      "accuracy:  83.15%; precision:   7.29%; recall:   1.06%; FB1:   1.85\n",
      "the best F is  1.85\n",
      "              LOC: precision:   0.00%; recall:   0.00%; FB1:   0.00  4\n",
      "             MISC: precision:   0.76%; recall:   0.33%; FB1:   0.46  393\n",
      "              ORG: precision:   4.93%; recall:   0.82%; FB1:   1.41  223\n",
      "              PER: precision:  20.08%; recall:   2.66%; FB1:   4.70  244\n",
      "processed 46435 tokens with 5648 phrases; found: 1123 phrases; correct: 98.\n",
      "accuracy:  82.38%; precision:   8.73%; recall:   1.74%; FB1:   2.89\n",
      "the best F is  2.89\n",
      "              LOC: precision:  45.45%; recall:   0.30%; FB1:   0.60  11\n",
      "             MISC: precision:   0.80%; recall:   0.57%; FB1:   0.66  502\n",
      "              ORG: precision:   5.18%; recall:   0.78%; FB1:   1.36  251\n",
      "              PER: precision:  21.17%; recall:   4.70%; FB1:   7.69  359\n",
      "********************************************************************************\n",
      "Epoch 7 Complete: Time Taken 19\n",
      "72 :  1.5041207207573786\n",
      "74 :  4.309536743164062\n",
      "76 :  0.8541719170026881\n",
      "78 :  0.3284991455078125\n",
      "80 :  0.6455921298288471\n",
      "processed 50145 tokens with 6019 phrases; found: 2036 phrases; correct: 227.\n",
      "accuracy:  82.63%; precision:  11.15%; recall:   3.77%; FB1:   5.64\n",
      "the best F is  5.64\n",
      "              LOC: precision:  62.50%; recall:   0.48%; FB1:   0.95  16\n",
      "             MISC: precision:   3.02%; recall:   2.77%; FB1:   2.89  861\n",
      "              ORG: precision:   6.15%; recall:   1.64%; FB1:   2.60  325\n",
      "              PER: precision:  20.50%; recall:   9.64%; FB1:  13.11  834\n",
      "processed 51362 tokens with 5942 phrases; found: 1769 phrases; correct: 193.\n",
      "accuracy:  83.33%; precision:  10.91%; recall:   3.25%; FB1:   5.01\n",
      "the best F is  5.01\n",
      "              LOC: precision:  33.33%; recall:   0.27%; FB1:   0.54  15\n",
      "             MISC: precision:   1.91%; recall:   1.41%; FB1:   1.62  680\n",
      "              ORG: precision:   3.23%; recall:   0.75%; FB1:   1.21  310\n",
      "              PER: precision:  21.60%; recall:   8.96%; FB1:  12.66  764\n",
      "processed 46435 tokens with 5648 phrases; found: 2163 phrases; correct: 232.\n",
      "accuracy:  82.50%; precision:  10.73%; recall:   4.11%; FB1:   5.94\n",
      "the best F is  5.94\n",
      "              LOC: precision:  42.50%; recall:   1.02%; FB1:   1.99  40\n",
      "             MISC: precision:   2.80%; recall:   3.42%; FB1:   3.08  858\n",
      "              ORG: precision:   9.19%; recall:   1.57%; FB1:   2.67  283\n",
      "              PER: precision:  16.80%; recall:  10.20%; FB1:  12.70  982\n",
      "********************************************************************************\n",
      "Epoch 8 Complete: Time Taken 19\n",
      "82 :  1.2468685838911269\n",
      "84 :  3.8194534301757814\n",
      "86 :  0.8438110948191593\n",
      "88 :  0.3186429214477539\n",
      "90 :  0.6720319217696613\n",
      "processed 50145 tokens with 6019 phrases; found: 1704 phrases; correct: 139.\n",
      "accuracy:  82.44%; precision:   8.16%; recall:   2.31%; FB1:   3.60\n",
      "              LOC: precision:  75.00%; recall:   0.43%; FB1:   0.86  12\n",
      "             MISC: precision:   3.32%; recall:   4.47%; FB1:   3.81  1266\n",
      "              ORG: precision:   9.38%; recall:   0.25%; FB1:   0.48  32\n",
      "              PER: precision:  21.57%; recall:   4.79%; FB1:   7.84  394\n",
      "processed 51362 tokens with 5942 phrases; found: 1412 phrases; correct: 107.\n",
      "accuracy:  83.15%; precision:   7.58%; recall:   1.80%; FB1:   2.91\n",
      "              LOC: precision:  80.00%; recall:   0.22%; FB1:   0.43  5\n",
      "             MISC: precision:   2.81%; recall:   3.04%; FB1:   2.92  998\n",
      "              ORG: precision:   5.00%; recall:   0.15%; FB1:   0.29  40\n",
      "              PER: precision:  19.78%; recall:   3.96%; FB1:   6.60  369\n",
      "processed 46435 tokens with 5648 phrases; found: 1879 phrases; correct: 167.\n",
      "accuracy:  82.35%; precision:   8.89%; recall:   2.96%; FB1:   4.44\n",
      "              LOC: precision:  81.25%; recall:   0.78%; FB1:   1.54  16\n",
      "             MISC: precision:   3.63%; recall:   6.55%; FB1:   4.67  1267\n",
      "              ORG: precision:  12.90%; recall:   0.24%; FB1:   0.47  31\n",
      "              PER: precision:  18.41%; recall:   6.43%; FB1:   9.53  565\n",
      "********************************************************************************\n",
      "Epoch 9 Complete: Time Taken 18\n",
      "92 :  1.2062878608703613\n",
      "94 :  2.9472203572591145\n",
      "96 :  1.1464705332050342\n",
      "98 :  0.4100333023071289\n",
      "100 :  0.561757382278737\n",
      "processed 50145 tokens with 6019 phrases; found: 2432 phrases; correct: 238.\n",
      "accuracy:  82.62%; precision:   9.79%; recall:   3.95%; FB1:   5.63\n",
      "              LOC: precision:  76.92%; recall:   0.48%; FB1:   0.95  13\n",
      "             MISC: precision:   4.73%; recall:   7.77%; FB1:   5.88  1543\n",
      "              ORG: precision:  10.26%; recall:   0.99%; FB1:   1.80  117\n",
      "              PER: precision:  18.84%; recall:   8.06%; FB1:  11.29  759\n",
      "processed 51362 tokens with 5942 phrases; found: 2118 phrases; correct: 190.\n",
      "accuracy:  83.26%; precision:   8.97%; recall:   3.20%; FB1:   4.71\n",
      "              LOC: precision:  80.00%; recall:   0.44%; FB1:   0.87  10\n",
      "             MISC: precision:   4.11%; recall:   5.64%; FB1:   4.76  1265\n",
      "              ORG: precision:   3.79%; recall:   0.37%; FB1:   0.68  132\n",
      "              PER: precision:  17.58%; recall:   6.79%; FB1:   9.79  711\n",
      "processed 46435 tokens with 5648 phrases; found: 2583 phrases; correct: 261.\n",
      "accuracy:  82.56%; precision:  10.10%; recall:   4.62%; FB1:   6.34\n",
      "the best F is  6.34\n",
      "              LOC: precision:  90.32%; recall:   1.68%; FB1:   3.30  31\n",
      "             MISC: precision:   4.55%; recall:   9.40%; FB1:   6.14  1449\n",
      "              ORG: precision:   8.74%; recall:   0.54%; FB1:   1.02  103\n",
      "              PER: precision:  15.80%; recall:   9.77%; FB1:  12.07  1000\n",
      "********************************************************************************\n",
      "Epoch 10 Complete: Time Taken 18\n",
      "102 :  0.8522824578815036\n",
      "104 :  2.5269139607747397\n",
      "106 :  0.4188832630742913\n",
      "108 :  0.2092966079711914\n",
      "110 :  0.44444938424010993\n",
      "processed 50145 tokens with 6019 phrases; found: 1711 phrases; correct: 285.\n",
      "accuracy:  83.11%; precision:  16.66%; recall:   4.74%; FB1:   7.37\n",
      "the best F is  7.37\n",
      "              LOC: precision:  95.31%; recall:   2.92%; FB1:   5.67  64\n",
      "             MISC: precision:   3.98%; recall:   3.51%; FB1:   3.73  830\n",
      "              ORG: precision:   7.18%; recall:   1.15%; FB1:   1.98  195\n",
      "              PER: precision:  28.46%; recall:   9.98%; FB1:  14.77  622\n",
      "processed 51362 tokens with 5942 phrases; found: 1514 phrases; correct: 213.\n",
      "accuracy:  83.65%; precision:  14.07%; recall:   3.58%; FB1:   5.71\n",
      "the best F is  5.71\n",
      "              LOC: precision:  94.34%; recall:   2.72%; FB1:   5.29  53\n",
      "             MISC: precision:   2.68%; recall:   1.95%; FB1:   2.26  671\n",
      "              ORG: precision:   2.51%; recall:   0.37%; FB1:   0.65  199\n",
      "              PER: precision:  23.69%; recall:   7.60%; FB1:  11.51  591\n",
      "processed 46435 tokens with 5648 phrases; found: 1915 phrases; correct: 309.\n",
      "accuracy:  83.06%; precision:  16.14%; recall:   5.47%; FB1:   8.17\n",
      "the best F is  8.17\n",
      "              LOC: precision:  82.69%; recall:   5.16%; FB1:   9.71  104\n",
      "             MISC: precision:   3.84%; recall:   4.42%; FB1:   4.11  807\n",
      "              ORG: precision:   7.39%; recall:   0.78%; FB1:   1.42  176\n",
      "              PER: precision:  21.62%; recall:  11.07%; FB1:  14.64  828\n",
      "********************************************************************************\n",
      "Epoch 11 Complete: Time Taken 18\n",
      "112 :  0.6482364071740044\n",
      "114 :  2.674613444010417\n",
      "116 :  0.6851593047409579\n",
      "118 :  0.11301753997802734\n",
      "120 :  0.5258242043749246\n",
      "processed 50145 tokens with 6019 phrases; found: 3105 phrases; correct: 756.\n",
      "accuracy:  84.00%; precision:  24.35%; recall:  12.56%; FB1:  16.57\n",
      "the best F is  16.57\n",
      "              LOC: precision:  68.78%; recall:   7.80%; FB1:  14.02  237\n",
      "             MISC: precision:   7.31%; recall:  10.11%; FB1:   8.48  1300\n",
      "              ORG: precision:  10.00%; recall:   2.55%; FB1:   4.06  310\n",
      "              PER: precision:  37.12%; recall:  26.32%; FB1:  30.80  1258\n",
      "processed 51362 tokens with 5942 phrases; found: 2922 phrases; correct: 749.\n",
      "accuracy:  84.69%; precision:  25.63%; recall:  12.61%; FB1:  16.90\n",
      "the best F is  16.9\n",
      "              LOC: precision:  73.62%; recall:   9.42%; FB1:  16.70  235\n",
      "             MISC: precision:   5.41%; recall:   6.40%; FB1:   5.86  1090\n",
      "              ORG: precision:   6.55%; recall:   1.42%; FB1:   2.33  290\n",
      "              PER: precision:  38.10%; recall:  27.04%; FB1:  31.63  1307\n",
      "processed 46435 tokens with 5648 phrases; found: 3226 phrases; correct: 731.\n",
      "accuracy:  83.92%; precision:  22.66%; recall:  12.94%; FB1:  16.48\n",
      "the best F is  16.48\n",
      "              LOC: precision:  45.17%; recall:  10.37%; FB1:  16.87  383\n",
      "             MISC: precision:   6.79%; recall:  11.68%; FB1:   8.59  1208\n",
      "              ORG: precision:  11.45%; recall:   1.81%; FB1:   3.12  262\n",
      "              PER: precision:  32.48%; recall:  27.58%; FB1:  29.83  1373\n",
      "********************************************************************************\n",
      "Epoch 12 Complete: Time Taken 18\n",
      "122 :  0.4327990743849013\n",
      "124 :  1.994152577718099\n",
      "126 :  0.6268590962548877\n",
      "128 :  0.26665313720703127\n",
      "130 :  0.32220214490264537\n",
      "processed 50145 tokens with 6019 phrases; found: 2087 phrases; correct: 388.\n",
      "accuracy:  83.37%; precision:  18.59%; recall:   6.45%; FB1:   9.57\n",
      "              LOC: precision:  86.49%; recall:   4.60%; FB1:   8.73  111\n",
      "             MISC: precision:   6.49%; recall:   8.40%; FB1:   7.32  1217\n",
      "              ORG: precision:   9.22%; recall:   2.22%; FB1:   3.58  293\n",
      "              PER: precision:  39.91%; recall:  10.48%; FB1:  16.61  466\n",
      "processed 51362 tokens with 5942 phrases; found: 1838 phrases; correct: 331.\n",
      "accuracy:  83.98%; precision:  18.01%; recall:   5.57%; FB1:   8.51\n",
      "              LOC: precision:  90.20%; recall:   5.01%; FB1:   9.49  102\n",
      "             MISC: precision:   4.98%; recall:   5.31%; FB1:   5.14  984\n",
      "              ORG: precision:   3.97%; recall:   0.89%; FB1:   1.46  302\n",
      "              PER: precision:  39.56%; recall:   9.66%; FB1:  15.53  450\n",
      "processed 46435 tokens with 5648 phrases; found: 2209 phrases; correct: 397.\n",
      "accuracy:  83.42%; precision:  17.97%; recall:   7.03%; FB1:  10.11\n",
      "              LOC: precision:  61.75%; recall:   6.77%; FB1:  12.21  183\n",
      "             MISC: precision:   5.37%; recall:   9.26%; FB1:   6.80  1210\n",
      "              ORG: precision:   5.81%; recall:   0.84%; FB1:   1.47  241\n",
      "              PER: precision:  35.65%; recall:  12.68%; FB1:  18.70  575\n",
      "********************************************************************************\n",
      "Epoch 13 Complete: Time Taken 19\n",
      "132 :  0.6885047488742404\n",
      "134 :  1.3424674987792968\n",
      "136 :  0.8618650911840176\n",
      "138 :  0.18666671752929687\n",
      "140 :  0.3680657419919047\n",
      "processed 50145 tokens with 6019 phrases; found: 2037 phrases; correct: 336.\n",
      "accuracy:  83.37%; precision:  16.49%; recall:   5.58%; FB1:   8.34\n",
      "              LOC: precision:  84.62%; recall:   4.21%; FB1:   8.03  104\n",
      "             MISC: precision:   5.52%; recall:   4.89%; FB1:   5.19  833\n",
      "              ORG: precision:   5.80%; recall:   3.29%; FB1:   4.20  690\n",
      "              PER: precision:  39.51%; recall:   9.13%; FB1:  14.84  410\n",
      "processed 51362 tokens with 5942 phrases; found: 1790 phrases; correct: 275.\n",
      "accuracy:  83.92%; precision:  15.36%; recall:   4.63%; FB1:   7.11\n",
      "              LOC: precision:  85.71%; recall:   4.57%; FB1:   8.68  98\n",
      "             MISC: precision:   3.03%; recall:   2.17%; FB1:   2.53  659\n",
      "              ORG: precision:   4.33%; recall:   2.09%; FB1:   2.82  646\n",
      "              PER: precision:  36.95%; recall:   7.76%; FB1:  12.83  387\n",
      "processed 46435 tokens with 5648 phrases; found: 2100 phrases; correct: 347.\n",
      "accuracy:  83.37%; precision:  16.52%; recall:   6.14%; FB1:   8.96\n",
      "              LOC: precision:  69.18%; recall:   6.06%; FB1:  11.14  146\n",
      "             MISC: precision:   3.58%; recall:   4.42%; FB1:   3.96  865\n",
      "              ORG: precision:   6.24%; recall:   2.11%; FB1:   3.15  561\n",
      "              PER: precision:  34.09%; recall:  11.13%; FB1:  16.78  528\n",
      "********************************************************************************\n",
      "Epoch 14 Complete: Time Taken 19\n",
      "142 :  0.5271536774105496\n",
      "144 :  1.098004659016927\n",
      "146 :  0.5999286908907624\n",
      "148 :  0.18700294494628905\n",
      "150 :  0.2834569717466141\n",
      "processed 50145 tokens with 6019 phrases; found: 2822 phrases; correct: 712.\n",
      "accuracy:  84.21%; precision:  25.23%; recall:  11.83%; FB1:  16.11\n",
      "              LOC: precision:  61.74%; recall:   9.19%; FB1:  16.00  311\n",
      "             MISC: precision:   8.64%; recall:  12.45%; FB1:  10.20  1354\n",
      "              ORG: precision:   9.55%; recall:   2.96%; FB1:   4.52  377\n",
      "              PER: precision:  47.05%; recall:  20.69%; FB1:  28.74  780\n",
      "processed 51362 tokens with 5942 phrases; found: 2555 phrases; correct: 639.\n",
      "accuracy:  84.84%; precision:  25.01%; recall:  10.75%; FB1:  15.04\n",
      "              LOC: precision:  61.37%; recall:   9.25%; FB1:  16.08  277\n",
      "             MISC: precision:   6.74%; recall:   8.13%; FB1:   7.37  1112\n",
      "              ORG: precision:   5.96%; recall:   1.72%; FB1:   2.66  386\n",
      "              PER: precision:  47.56%; recall:  20.14%; FB1:  28.30  780\n",
      "********************************************************************************\n",
      "Epoch 15 Complete: Time Taken 18\n",
      "152 :  0.5673758321338229\n",
      "154 :  1.7607748667399088\n",
      "156 :  0.25856091153470184\n",
      "158 :  0.13638534545898437\n",
      "160 :  0.3323520380557734\n",
      "processed 50145 tokens with 6019 phrases; found: 2266 phrases; correct: 548.\n",
      "accuracy:  83.69%; precision:  24.18%; recall:   9.10%; FB1:  13.23\n",
      "              LOC: precision:  53.37%; recall:  10.63%; FB1:  17.72  416\n",
      "             MISC: precision:   8.45%; recall:  12.55%; FB1:  10.10  1397\n",
      "              ORG: precision:  26.39%; recall:   1.56%; FB1:   2.95  72\n",
      "              PER: precision:  49.61%; recall:  10.65%; FB1:  17.54  381\n",
      "processed 51362 tokens with 5942 phrases; found: 1958 phrases; correct: 530.\n",
      "accuracy:  84.38%; precision:  27.07%; recall:   8.92%; FB1:  13.42\n",
      "              LOC: precision:  61.75%; recall:  13.45%; FB1:  22.08  400\n",
      "             MISC: precision:   9.24%; recall:  11.39%; FB1:  10.20  1136\n",
      "              ORG: precision:   6.00%; recall:   0.22%; FB1:   0.43  50\n",
      "              PER: precision:  47.04%; recall:   9.50%; FB1:  15.81  372\n",
      "processed 46435 tokens with 5648 phrases; found: 2411 phrases; correct: 533.\n",
      "accuracy:  83.65%; precision:  22.11%; recall:   9.44%; FB1:  13.23\n",
      "              LOC: precision:  33.66%; recall:  12.47%; FB1:  18.20  618\n",
      "             MISC: precision:   7.40%; recall:  13.39%; FB1:   9.53  1271\n",
      "              ORG: precision:  26.98%; recall:   1.02%; FB1:   1.97  63\n",
      "              PER: precision:  46.62%; recall:  13.23%; FB1:  20.62  459\n",
      "********************************************************************************\n",
      "Epoch 16 Complete: Time Taken 20\n",
      "162 :  0.37872738308376735\n",
      "164 :  1.5161626180013021\n",
      "166 :  0.3497602028347996\n",
      "168 :  0.12907066345214843\n",
      "170 :  0.2807232403847241\n",
      "processed 50145 tokens with 6019 phrases; found: 2814 phrases; correct: 694.\n",
      "accuracy:  83.97%; precision:  24.66%; recall:  11.53%; FB1:  15.71\n",
      "              LOC: precision:  45.04%; recall:  15.65%; FB1:  23.23  726\n",
      "             MISC: precision:   7.39%; recall:   7.98%; FB1:   7.67  1015\n",
      "              ORG: precision:   7.81%; recall:   3.70%; FB1:   5.02  576\n",
      "              PER: precision:  49.70%; recall:  13.92%; FB1:  21.75  497\n",
      "processed 51362 tokens with 5942 phrases; found: 2505 phrases; correct: 630.\n",
      "accuracy:  84.60%; precision:  25.15%; recall:  10.60%; FB1:  14.92\n",
      "              LOC: precision:  47.00%; recall:  17.04%; FB1:  25.01  666\n",
      "             MISC: precision:   4.63%; recall:   4.12%; FB1:   4.36  820\n",
      "              ORG: precision:   5.06%; recall:   1.94%; FB1:   2.80  514\n",
      "              PER: precision:  50.10%; recall:  13.74%; FB1:  21.56  505\n",
      "processed 46435 tokens with 5648 phrases; found: 3002 phrases; correct: 652.\n",
      "accuracy:  83.77%; precision:  21.72%; recall:  11.54%; FB1:  15.08\n",
      "              LOC: precision:  28.43%; recall:  16.79%; FB1:  21.11  985\n",
      "             MISC: precision:   6.29%; recall:   8.26%; FB1:   7.14  922\n",
      "              ORG: precision:   9.80%; recall:   3.01%; FB1:   4.61  510\n",
      "              PER: precision:  45.13%; recall:  16.33%; FB1:  23.98  585\n",
      "********************************************************************************\n",
      "Epoch 17 Complete: Time Taken 22\n",
      "172 :  0.23829799228244358\n",
      "174 :  0.5315423329671224\n",
      "176 :  0.28234111528592376\n",
      "178 :  0.1566854476928711\n",
      "180 :  0.28460867156393277\n",
      "processed 50145 tokens with 6019 phrases; found: 2559 phrases; correct: 711.\n",
      "accuracy:  84.16%; precision:  27.78%; recall:  11.81%; FB1:  16.58\n",
      "the best F is  16.58\n",
      "              LOC: precision:  53.60%; recall:  16.04%; FB1:  24.69  625\n",
      "             MISC: precision:   8.19%; recall:   8.83%; FB1:   8.50  1013\n",
      "              ORG: precision:   9.64%; recall:   2.63%; FB1:   4.13  332\n",
      "              PER: precision:  44.31%; recall:  14.71%; FB1:  22.09  589\n",
      "processed 51362 tokens with 5942 phrases; found: 2291 phrases; correct: 613.\n",
      "accuracy:  84.70%; precision:  26.76%; recall:  10.32%; FB1:  14.89\n",
      "              LOC: precision:  53.94%; recall:  16.39%; FB1:  25.14  558\n",
      "             MISC: precision:   5.58%; recall:   5.10%; FB1:   5.33  843\n",
      "              ORG: precision:   4.46%; recall:   1.12%; FB1:   1.79  336\n",
      "              PER: precision:  45.13%; recall:  13.57%; FB1:  20.87  554\n",
      "processed 46435 tokens with 5648 phrases; found: 2777 phrases; correct: 666.\n",
      "accuracy:  84.03%; precision:  23.98%; recall:  11.79%; FB1:  15.81\n",
      "              LOC: precision:  33.91%; recall:  17.69%; FB1:  23.25  870\n",
      "             MISC: precision:   7.00%; recall:   9.26%; FB1:   7.98  928\n",
      "              ORG: precision:  10.31%; recall:   1.81%; FB1:   3.07  291\n",
      "              PER: precision:  40.12%; recall:  17.07%; FB1:  23.95  688\n",
      "********************************************************************************\n",
      "Epoch 18 Complete: Time Taken 19\n",
      "182 :  0.18123682339986166\n",
      "184 :  0.9881629943847656\n",
      "186 :  0.3210570931085044\n",
      "188 :  0.15239830017089845\n",
      "190 :  0.28098986415789395\n",
      "processed 50145 tokens with 6019 phrases; found: 2576 phrases; correct: 579.\n",
      "accuracy:  83.80%; precision:  22.48%; recall:   9.62%; FB1:  13.47\n",
      "              LOC: precision:  71.12%; recall:   7.90%; FB1:  14.22  232\n",
      "             MISC: precision:   9.53%; recall:  18.09%; FB1:  12.49  1783\n",
      "              ORG: precision:  23.81%; recall:   1.64%; FB1:   3.08  84\n",
      "              PER: precision:  46.96%; recall:  12.63%; FB1:  19.90  477\n",
      "processed 51362 tokens with 5942 phrases; found: 2250 phrases; correct: 534.\n",
      "accuracy:  84.46%; precision:  23.73%; recall:   8.99%; FB1:  13.04\n",
      "              LOC: precision:  79.70%; recall:   8.76%; FB1:  15.79  202\n",
      "             MISC: precision:   9.92%; recall:  16.27%; FB1:  12.33  1512\n",
      "              ORG: precision:   3.61%; recall:   0.22%; FB1:   0.42  83\n",
      "              PER: precision:  48.57%; recall:  11.94%; FB1:  19.17  453\n",
      "processed 46435 tokens with 5648 phrases; found: 2765 phrases; correct: 581.\n",
      "accuracy:  83.70%; precision:  21.01%; recall:  10.29%; FB1:  13.81\n",
      "              LOC: precision:  61.30%; recall:   9.59%; FB1:  16.59  261\n",
      "             MISC: precision:   7.76%; recall:  20.51%; FB1:  11.26  1855\n",
      "              ORG: precision:  20.99%; recall:   1.02%; FB1:   1.95  81\n",
      "              PER: precision:  45.77%; recall:  16.08%; FB1:  23.80  568\n",
      "********************************************************************************\n",
      "Epoch 19 Complete: Time Taken 18\n",
      "192 :  0.2570394145117866\n",
      "194 :  1.0000741322835287\n",
      "196 :  0.2512715359237536\n",
      "198 :  0.09056468963623046\n",
      "200 :  0.2026991678480936\n",
      "processed 50145 tokens with 6019 phrases; found: 3440 phrases; correct: 973.\n",
      "accuracy:  84.96%; precision:  28.28%; recall:  16.17%; FB1:  20.57\n",
      "the best F is  20.57\n",
      "              LOC: precision:  58.06%; recall:  12.92%; FB1:  21.14  465\n",
      "             MISC: precision:   8.78%; recall:  11.81%; FB1:  10.07  1264\n",
      "              ORG: precision:   7.94%; recall:   4.28%; FB1:   5.56  655\n",
      "              PER: precision:  51.14%; recall:  30.44%; FB1:  38.16  1056\n",
      "processed 51362 tokens with 5942 phrases; found: 3153 phrases; correct: 920.\n",
      "accuracy:  85.63%; precision:  29.18%; recall:  15.48%; FB1:  20.23\n",
      "the best F is  20.23\n",
      "              LOC: precision:  61.78%; recall:  13.99%; FB1:  22.81  416\n",
      "             MISC: precision:   6.61%; recall:   7.48%; FB1:   7.02  1044\n",
      "              ORG: precision:   5.93%; recall:   2.83%; FB1:   3.83  641\n",
      "              PER: precision:  52.85%; recall:  30.18%; FB1:  38.42  1052\n",
      "processed 46435 tokens with 5648 phrases; found: 3623 phrases; correct: 880.\n",
      "accuracy:  84.58%; precision:  24.29%; recall:  15.58%; FB1:  18.98\n",
      "the best F is  18.98\n",
      "              LOC: precision:  42.04%; recall:  13.61%; FB1:  20.56  540\n",
      "             MISC: precision:   5.91%; recall:  11.40%; FB1:   7.78  1354\n",
      "              ORG: precision:  12.13%; recall:   4.46%; FB1:   6.52  610\n",
      "              PER: precision:  44.59%; recall:  30.86%; FB1:  36.48  1119\n",
      "********************************************************************************\n",
      "Epoch 20 Complete: Time Taken 19\n",
      "202 :  0.3367764949798584\n",
      "204 :  0.6817824045817057\n",
      "206 :  0.21255269428152493\n",
      "208 :  0.11123374938964845\n",
      "210 :  0.08994842588211119\n",
      "processed 50145 tokens with 6019 phrases; found: 3472 phrases; correct: 1305.\n",
      "accuracy:  85.32%; precision:  37.59%; recall:  21.68%; FB1:  27.50\n",
      "the best F is  27.5\n",
      "              LOC: precision:  46.72%; recall:  36.48%; FB1:  40.97  1631\n",
      "             MISC: precision:  10.05%; recall:   7.98%; FB1:   8.90  746\n",
      "              ORG: precision:  10.12%; recall:   2.71%; FB1:   4.28  326\n",
      "              PER: precision:  56.57%; recall:  24.52%; FB1:  34.21  769\n",
      "processed 51362 tokens with 5942 phrases; found: 3117 phrases; correct: 1172.\n",
      "accuracy:  85.86%; precision:  37.60%; recall:  19.72%; FB1:  25.87\n",
      "the best F is  25.87\n",
      "              LOC: precision:  47.95%; recall:  35.60%; FB1:  40.86  1364\n",
      "             MISC: precision:   6.36%; recall:   4.66%; FB1:   5.38  676\n",
      "              ORG: precision:   9.85%; recall:   2.01%; FB1:   3.34  274\n",
      "              PER: precision:  55.79%; recall:  24.32%; FB1:  33.88  803\n",
      "processed 46435 tokens with 5648 phrases; found: 3662 phrases; correct: 1129.\n",
      "accuracy:  84.79%; precision:  30.83%; recall:  19.99%; FB1:  24.25\n",
      "the best F is  24.25\n",
      "              LOC: precision:  33.56%; recall:  35.01%; FB1:  34.27  1740\n",
      "             MISC: precision:   8.41%; recall:   9.54%; FB1:   8.94  797\n",
      "              ORG: precision:  16.56%; recall:   3.07%; FB1:   5.18  308\n",
      "              PER: precision:  52.26%; recall:  26.41%; FB1:  35.09  817\n",
      "********************************************************************************\n",
      "Epoch 21 Complete: Time Taken 19\n",
      "212 :  0.06972514258490668\n",
      "214 :  0.5497151692708333\n",
      "216 :  0.16673826215786902\n",
      "218 :  0.10462623596191406\n",
      "220 :  0.2780440783408618\n",
      "processed 50145 tokens with 6019 phrases; found: 3113 phrases; correct: 1115.\n",
      "accuracy:  85.09%; precision:  35.82%; recall:  18.52%; FB1:  24.42\n",
      "              LOC: precision:  55.65%; recall:  27.57%; FB1:  36.88  1035\n",
      "             MISC: precision:  10.97%; recall:  12.98%; FB1:  11.89  1112\n",
      "              ORG: precision:  12.66%; recall:   2.47%; FB1:   4.13  237\n",
      "              PER: precision:  53.09%; recall:  21.82%; FB1:  30.92  729\n",
      "processed 51362 tokens with 5942 phrases; found: 2831 phrases; correct: 1028.\n",
      "accuracy:  85.64%; precision:  36.31%; recall:  17.30%; FB1:  23.44\n",
      "              LOC: precision:  58.48%; recall:  28.14%; FB1:  38.00  884\n",
      "             MISC: precision:   8.36%; recall:   9.00%; FB1:   8.67  993\n",
      "              ORG: precision:  11.11%; recall:   1.72%; FB1:   2.97  207\n",
      "              PER: precision:  54.22%; recall:  21.99%; FB1:  31.29  747\n",
      "processed 46435 tokens with 5648 phrases; found: 3305 phrases; correct: 996.\n",
      "accuracy:  84.75%; precision:  30.14%; recall:  17.63%; FB1:  22.25\n",
      "              LOC: precision:  38.08%; recall:  27.10%; FB1:  31.66  1187\n",
      "             MISC: precision:   8.93%; recall:  13.96%; FB1:  10.89  1097\n",
      "              ORG: precision:  14.48%; recall:   1.93%; FB1:   3.40  221\n",
      "              PER: precision:  51.75%; recall:  25.60%; FB1:  34.26  800\n",
      "********************************************************************************\n",
      "Epoch 22 Complete: Time Taken 18\n",
      "222 :  0.49953065978156197\n",
      "224 :  0.3181989034016927\n",
      "226 :  0.12761800846163246\n",
      "228 :  0.013621673583984376\n",
      "230 :  0.10841631060861712\n",
      "processed 50145 tokens with 6019 phrases; found: 3309 phrases; correct: 1196.\n",
      "accuracy:  85.52%; precision:  36.14%; recall:  19.87%; FB1:  25.64\n",
      "              LOC: precision:  63.24%; recall:  25.04%; FB1:  35.87  827\n",
      "             MISC: precision:  13.73%; recall:  17.02%; FB1:  15.20  1165\n",
      "              ORG: precision:  11.28%; recall:   3.12%; FB1:   4.89  337\n",
      "              PER: precision:  48.47%; recall:  26.78%; FB1:  34.50  980\n",
      "processed 51362 tokens with 5942 phrases; found: 3066 phrases; correct: 1138.\n",
      "accuracy:  86.10%; precision:  37.12%; recall:  19.15%; FB1:  25.27\n",
      "              LOC: precision:  64.53%; recall:  26.35%; FB1:  37.42  750\n",
      "             MISC: precision:  12.21%; recall:  13.77%; FB1:  12.95  1040\n",
      "              ORG: precision:  10.54%; recall:   2.31%; FB1:   3.79  294\n",
      "              PER: precision:  50.51%; recall:  26.93%; FB1:  35.13  982\n",
      "processed 46435 tokens with 5648 phrases; found: 3501 phrases; correct: 1102.\n",
      "accuracy:  85.27%; precision:  31.48%; recall:  19.51%; FB1:  24.09\n",
      "              LOC: precision:  45.29%; recall:  27.40%; FB1:  34.14  1009\n",
      "             MISC: precision:  11.41%; recall:  17.66%; FB1:  13.86  1087\n",
      "              ORG: precision:  14.92%; recall:   2.83%; FB1:   4.76  315\n",
      "              PER: precision:  43.49%; recall:  29.31%; FB1:  35.02  1090\n",
      "********************************************************************************\n",
      "Epoch 23 Complete: Time Taken 18\n",
      "232 :  0.11347018347846137\n",
      "234 :  0.46506271362304685\n",
      "236 :  0.1894932184750733\n",
      "238 :  0.07763381958007812\n",
      "240 :  0.1703701976643566\n",
      "processed 50145 tokens with 6019 phrases; found: 3785 phrases; correct: 1405.\n",
      "accuracy:  85.66%; precision:  37.12%; recall:  23.34%; FB1:  28.66\n",
      "the best F is  28.66\n",
      "              LOC: precision:  50.41%; recall:  35.28%; FB1:  41.51  1462\n",
      "             MISC: precision:  14.77%; recall:  15.43%; FB1:  15.09  982\n",
      "              ORG: precision:  12.58%; recall:   4.93%; FB1:   7.09  477\n",
      "              PER: precision:  53.59%; recall:  26.10%; FB1:  35.10  864\n",
      "processed 51362 tokens with 5942 phrases; found: 3470 phrases; correct: 1264.\n",
      "accuracy:  86.15%; precision:  36.43%; recall:  21.27%; FB1:  26.86\n",
      "the best F is  26.86\n",
      "              LOC: precision:  49.84%; recall:  34.30%; FB1:  40.63  1264\n",
      "             MISC: precision:  11.76%; recall:  11.17%; FB1:  11.46  876\n",
      "              ORG: precision:  11.01%; recall:   3.65%; FB1:   5.49  445\n",
      "              PER: precision:  54.46%; recall:  26.17%; FB1:  35.35  885\n",
      "processed 46435 tokens with 5648 phrases; found: 4015 phrases; correct: 1287.\n",
      "accuracy:  85.49%; precision:  32.05%; recall:  22.79%; FB1:  26.64\n",
      "the best F is  26.64\n",
      "              LOC: precision:  38.36%; recall:  37.65%; FB1:  38.00  1637\n",
      "             MISC: precision:  12.17%; recall:  15.81%; FB1:  13.75  912\n",
      "              ORG: precision:  17.01%; recall:   5.00%; FB1:   7.72  488\n",
      "              PER: precision:  47.55%; recall:  28.76%; FB1:  35.84  978\n",
      "********************************************************************************\n",
      "Epoch 24 Complete: Time Taken 19\n",
      "242 :  0.0604452027214898\n",
      "244 :  0.2820638020833333\n",
      "246 :  0.08957073252688172\n",
      "248 :  0.054420242309570314\n",
      "250 :  0.15958174775466033\n",
      "processed 50145 tokens with 6019 phrases; found: 3273 phrases; correct: 1200.\n",
      "accuracy:  85.16%; precision:  36.66%; recall:  19.94%; FB1:  25.83\n",
      "              LOC: precision:  57.94%; recall:  28.48%; FB1:  38.19  1027\n",
      "             MISC: precision:  15.36%; recall:  22.23%; FB1:  18.17  1361\n",
      "              ORG: precision:  12.59%; recall:   2.88%; FB1:   4.69  278\n",
      "              PER: precision:  59.47%; recall:  20.35%; FB1:  30.32  607\n",
      "processed 51362 tokens with 5942 phrases; found: 2953 phrases; correct: 1083.\n",
      "accuracy:  85.66%; precision:  36.67%; recall:  18.23%; FB1:  24.35\n",
      "              LOC: precision:  59.18%; recall:  28.96%; FB1:  38.89  899\n",
      "             MISC: precision:  13.43%; recall:  17.90%; FB1:  15.34  1229\n",
      "              ORG: precision:  11.67%; recall:   2.09%; FB1:   3.54  240\n",
      "              PER: precision:  61.20%; recall:  19.44%; FB1:  29.50  585\n",
      "processed 46435 tokens with 5648 phrases; found: 3440 phrases; correct: 1112.\n",
      "accuracy:  84.93%; precision:  32.33%; recall:  19.69%; FB1:  24.47\n",
      "              LOC: precision:  43.99%; recall:  30.94%; FB1:  36.33  1173\n",
      "             MISC: precision:  12.59%; recall:  23.65%; FB1:  16.43  1319\n",
      "              ORG: precision:  19.42%; recall:   2.83%; FB1:   4.94  242\n",
      "              PER: precision:  54.25%; recall:  23.69%; FB1:  32.97  706\n",
      "********************************************************************************\n",
      "Epoch 25 Complete: Time Taken 21\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, optimizer, result_path, model_name, usedataset=opt.dataset, mappings= mappings) \n",
    "losses, all_F = trainer.train_model(opt.num_epochs, train_data, dev_data, test_train_data, test_data,\n",
    "                                     learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "from neural_ner.util.loader import Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "parameters['zeros'] = 0\n",
    "parameters['lower'] = 1\n",
    "parameters['wrdim'] = 100\n",
    "parameters['ptrnd'] = 'wordvectors/glove.6B.100d.txt'\n",
    "parameters['tgsch'] = 'iobes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23511 unique words (2200865 in total)\n",
      "Loading pretrained embeddings from wordvectors/glove.6B.100d.txt...\n",
      "Found 118 unique characters\n",
      "Found 75 unique named entity tags\n",
      "115812 / 12217 / 15680 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data, mappings = loader.load_ontonotes('datasets/ontonotes/',parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for data in train_data:\n",
    "    for word in data['str_words']:\n",
    "        prediction.append(word)\n",
    "with open('hawa.txt', 'wb') as f:\n",
    "    f.write('\\n'.join(prediction).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= torch.randn(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose(1,2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.return_types.max' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4fd8fef077ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.return_types.max' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "a.max(2).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.autograd.Variable(torch.randn(3,5)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.cpu().data.numpy() * np.random.randn(3,5,4).max(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
